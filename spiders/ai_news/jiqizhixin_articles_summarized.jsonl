{"title": "AI下半场的「Game Changer」，直让老外惊呼「Amazing」", "published_at": "2025-07-14 20:41:33", "url": "https://www.jiqizhixin.com/articles/2025-07-14-11", "content": "又一项中国的 AI 技术在国外火了！\n近日，海外社交媒体平台 X 上众多关注 AI 的博主对一个来自中国的新技术展开热烈讨论。\n有人表示：「中国不是随便玩玩。这事儿影响太大了！」\n有的直呼：「中国真的是在突破边界！」\n还有的说：「中国不是在『下棋』，他们在重新定义整个『棋局』！」\n到底是什么样的技术，竟能让一众老外给出如此之高的评价？\n还惊呼「Amazing」「Superb」「Exciting」（小编仿佛在做雅思考试的高级词汇替代练习）。\n头部 AI 科技博主 Jaynit Makwana 发帖说：「......It's called AI Flow - a system where models adapt, collaborate, and deploy......」\n科技博主 Rishabh 推文表示：「......（它）可能会重塑生成式人工智能在边缘端的运行方式...... 比我们见过的任何技术都更快、更经济、更智能......」\nRasel Hosen 回复评论说：「...... 拥抱一个人工智能与我们的生活无缝融合的未来，真的可能彻底改变协作模式。已经迫不及待想看看它会如何发展了！」\nMuhammad Ayan 表示：「这正是我们在实时人工智能部署中所需要的那种架构。」\nVibeEdge 更是用「\nGame Changer\n」来形容。\n小编立即搜索了一下，找到了\nAI Flow\n的定义，并且它还有个中文名字——\n智传网\n。\n智传网（AI Flow）是人工智能与通信网络交叉领域的一项关键技术，即\n通过网络分层架构，基于智能体间的连接以及智能体和人的交互，实现智能的传递和涌现\n。\n通过智传网（AI Flow），智能可以突破设备和平台的限制，在网络不同层之间自由流动，从云计算中心到终端设备，实现随需响应，随处而至。\n更令小编没想到的是，这个技术竟是出自中国的一家央企 ——\n中国电信\n。\n根据 AI 科技博主 EyeingAI 介绍：「AI Flow by Professor Xuelong Li (CTO at China Telecom and Director of TeleAI) and the team explores how AI can actually work better in the real world.」\n原来，智传网（AI Flow）是\n中国电信人工智能研究院（TeleAI）\n正在着重发力的一项技术，由其院长\n李学龙教授\n带领团队打造。\n李学龙教授是中国电信集团 CTO、首席科学家，他是全球少有的光电和人工智能双领域专家，在光电领域的 OSA（美国光学学会）、SPIE（国际光学工程学会）和人工智能领域的 AAAI、AAAS、ACM 学会，以及 IEEE，都入选了 Fellow。\n而这些海外博主们之所以会关注到智传网（AI Flow），是源于 TeleAI 团队于 6 月中旬在 arXiv 上挂出的一份前沿技术报告：\nAI Flow: Perspectives, Scenarios, and Approaches\n报告地址：https://arxiv.org/abs/2506.12479\n在这份技术报告挂出后，快速受到全球技术市场研究咨询机构 Omdia 的关注，还发布了一份行业短评报告，在分析生成式人工智能技术落地应用的趋势和方向时，推荐产业各方将 TeleAI 的智传网（AI Flow）技术「On the Radar」。\nOmdia 的 AI 首席分析师苏廉节（Lian Jye Su）还在社交媒体平台发布推文表示：\n「通过架起信息技术与通信技术之间的桥梁，智传网（AI Flow）为自动驾驶汽车、无人机和人形机器人等资源密集型应用提供了强大支持，同时不会在延迟、隐私或性能方面做出妥协。分布式智能的未来已然来临 —— 在这一未来中，先进应用既能突破设备限制，又能保持实时响应能力与数据安全性。」\nAI Flow 到底是什么？又为什么需要它？\n翻开技术报告，开篇提到了两个赫赫有名的人物：Claude Shannon（克劳德・香农）和 Alan Turing（艾伦・图灵），一位是信息论的创始人，一位被誉为计算机科学之父。他们分别奠定了信息技术（IT）与通信技术（CT）的基础。\n报告指出，IT 与 CT 的发展呈现出双轨并行的态势，一方面不断提升单个机器的性能，另一方面构建网络以实现多台机器间更高效的互联。这种协同效应引发了一场技术革命，如今在人工智能大模型的推动下达到顶峰。\nAI 的能力边界正以超乎人们想象的速度扩张，文能赋诗作画写代码，武能驱动机器人、无人机与自动驾驶汽车。更有观点认为我们正在进入所谓的「AI 下半场」。然而，大模型对资源消耗大和通信带宽高的需求，在实现普适智能方面正面临着巨大挑战。\n真正的现实是，除了在聊天框里与 AI 对话，我们手中的手机、佩戴的设备、驾驶的汽车，距离真正的「泛在智能」似乎仍有遥远的距离。\n于是，一个巨大的悖论也随之浮现：\n既然 AI 已如此强大，为何它仍未能无缝融入我们日常生活的方方面面呢？\n答案其实就隐藏在 AI 强大的外表之下。一个残酷的现实是：几乎所有顶尖的 AI 都无法直接运行在我们身边的终端设备上。它们是名副其实的「云端巨兽」，严重依赖远在千里之外、拥有庞大算力的数据中心。\n举个例子，如果你要运行 671B 参数量的 DeepSeek-R1 模型（BF16 满血版），则理论上至少需要 1342 GB 内存，而要保证 Token 输出速度，所需的算力更是让人咋舌。很明显，这些需求已经远远超出了绝大多数手机、汽车等端侧设备的承载极限。\n这种绝对的云端依赖为 AI 应用的普及带来了最致命的枷锁：\n延迟\n。\n正如英特尔前 CEO 帕特・基辛格所言：「如果我必须将数据发送到云再回来，它的响应速度永远不可能像我在本地处理那样快。」—— 这是不可违背的「物理定律」。\n对于毫秒必争的自动驾驶汽车以及要求实时响应的外科手术机器人，这种延迟是不可接受的，甚至是生死攸关的。\n这便是 AI 普及的「最后一公里」困局：最需要即时智能的场景往往离云端很远；而最强大的智能，又偏偏被困在云端，无法下来。\n如何打破这个僵局？过去，行业的思路是造更快的芯片、建更大的数据中心，但这越来越像一场投入产出比急剧下降的「军备竞赛」。\n当所有人都执着于如何把算力的砖墙砌得更高时，破局的答案或许来自一个长期被忽视、却更关乎万物互联本质的领域——\n通信\n。\n智传网（AI Flow）\n正是这个颠覆性的答案！\n它是一套整合了通信网络与 AI 模型的创新架构，目标是要搭建起一座桥梁，让智能本身能够突破平台的限制，在「端、边、云」的层级化架构之间像数据一样自由流动、随需而至，实现 Ubiquitous AI Applications（让 AI 应用无处不在）。\n就像它的中文名字一样，「智」代表人工智能，「传」代表通信，「网」代表网络，是一座\n让「智」能「传」输之「网」\n。\n仔细看过 TeleAI 的技术报告后发现，智传网（AI Flow）是一套组合拳，包含三个核心技术方向。\n端-边-云协同\n（Device-Edge-Cloud Collaboration）：为智能的分布式运行提供了硬件基础。\n家族式同源模型\n（Familial Model）：能够灵活伸缩以适应不同设备，并通过复用计算结果实现高效协作。\n基于连接与交互的智能涌现\n（Connectivity- and Interaction-based Intelligence Emergence）：通过模型间的连接与交互，催生出超越任何单体能力的智能涌现，达成 1+1>2 的效果。\n端-边-云协同  分布式推理\n为了实现 AI 服务的增强智能和及时响应，智传网（AI Flow）采用了分层式端-边-云协同架构。这三层网络架构可为各种下游任务提供灵活的分布式推理工作流程，是模型协作的基础，而模型协作正是智传网（AI Flow）的一大基石。\n首先来看现今通信网络普遍使用的三层网络架构，即\n设备层（端）、边缘层（边）\n和\n云层（云）\n。\n其中，端侧设备通信时延最短但算力很低；部署在基站（BS）和路侧单元（RSU）等边缘节点的服务器算力稍强但通信时延稍长，而云端服务器虽然算力很强，但\n因为网络路由，\n通信时延最高。\n边缘节点由于靠近终端设备，因此能够提供中等计算能力和相对较低的传输延迟。边缘服务器可充当云层和设备层之间的中介，支持本地化处理和动态任务编排。通过从资源受限的终端设备接管对延迟敏感的工作负载，边缘层可以提高响应速度，同时减少对远程云基础设施的依赖。\n然而，与云集群相比，其硬件资源仍然有限。因此，边缘服务器对于工作负载的动态编排至关重要，它可以将计算密集型操作卸载到云端集群，同时直接支持终端层设备，从而确保高效利用分层资源。\n容易看出，对于这种架构，有效的动态任务编排至关重要。\n为了做到这一点，针对端-边的协同推理，TeleAI 提出了\n任务导向型特征压缩\n（Task-Oriented Feature Compression）方法，简称\nTOFC\n。该方法可通过在设备上执行融合与压缩，根据通道条件动态优化与任务相关的多模态特征传输。\n这种方式能极大减少传输的数据量，在实验中，相比传统图片压缩方式，TOFC 能在保证任务效果的同时，节省高达 60% 的传输数据。\n用于端-边的协同推理的 TOFC 系统图示\n具体来说，如上图所示，首先由 CLIP 视觉编码器生成视觉特征并对其进行基于 K 最近邻的密度峰值聚类（DPC-KNN），从而大幅减少数据量和计算负载。\n然后，采用基于超先验的熵模型对融合后的特征进行编码和解码，从而在保持下游任务性能的同时最大限度地减少数据传输。\n最后，训练多个专门用于编码不同特征的熵模型，并根据输入特征的特点自适应地选择最优熵模型。\n此外，为了进一步提升效率，智传网（TeleAI）还整合了\n推测解码\n（speculative decoding）技术，也就是使用「Draft Token 生成 + 验证」的方法。当用户发起请求时：\n设备先「生成 Draft Tokens」\n：部署在手机等终端设备上的轻量级模型会利用其响应速度快的优势，迅速生成回答的「Draft Tokens」。\n云/边后「验证」\n：「Draft Tokens」生成后，会被发送到边缘服务器或云端。部署在那里的、能力更强的大模型并不会从头重新生成一遍答案，而是扮演「验证者」的角色，快速地验证和修正「Draft Tokens」中的错误或不完善之处。\n通过推测解码实现的设备与边缘服务器的分层协作框架概览\n为了克服传统推测解码中顺序式「Draft Token 生成 + 验证」范式所导致的固有延迟，TeleAI 提出了一种并行式端-边协作解码框架。而且该框架非常灵活，可以轻松地扩展成「端-边-云」三层架构，解决一些更为复杂的任务，如下图所示。\n「端-边」两层以及「端-边-云」三层的协同解码示意图\n这种模式下，用户能以小模型的速度享受到大模型的质量。实验证明，在数学推理、代码生成等任务上，这种协同方式的生成速度比单独使用云端大模型提升了约 25%，同时还能保证与大模型同等的准确度 。\n家族式同源模型  如何定制不同大小的智能？\n家族式同源模型是指一系列大小不同但隐含特征已对齐的模型，因此可以实现无开销的信息共享和有效协作。\n实际上，这套模型并非不同大小模型的简单组合，也不是像混合专家（MoE）模型那样随机激活一定比例的参数，而是能像变焦镜头一样灵活伸缩，让一个大模型可以按需「变身」成不同尺寸，以适应各类终端的算力限制。\n更关键的是，它们在协同工作时还能够复用彼此的计算结果，从而避免重复劳动，极大提升效率。不仅如此，该架构支持几乎任意数量参数的模型，使其能够充分利用异构设备的计算能力，从而满足各种下游任务的需求。\n实现家族式同源模型的两大核心策略分别是：\n权重分解\n（Weight Decomposition）：将模型中庞大的参数矩阵分解为多个更小的矩阵，从而在不破坏结构的情况下，精细地调整模型大小。在这方面，TeleAI 新提出了一种名为\n分层主成分分解（HPCD）\n的技术，可通过对 Transformer 模块内的线性层进行自适应权重分解，实现对总参数数量进行细粒度调整。\n早退出\n（Early Exit）：允许模型在计算过程中，根据任务的难易程度，从中间的某一层「提前」产生结果，而不必「跑完全程」。在这方面，TeleAI 新提出的了一种名为\n使用可扩展分支的早退出（EESB）\n的技术，可通过仔细调整已分解的层之间隐藏特征的维度，家族式同源模型可以实现几乎任意数量的参数，从而适应异构设备的硬件能力。\nTeleAI 新提出的 EESB 早退出方法的示意图\n这种设计的最大优势在于计算的复用与接力。由于小尺寸模型本质上是家族式同源模型的一个「子集」，当终端设备用 3B 大小的分支完成初步计算后，如果需要更强的智能，它可以将计算的中间结果无缝传递给边缘服务器上的 7B 分支。服务器接收后，无需从头开始，可以直接在 3B 的计算基础上继续向后推理。这种「计算接力」可避免重复劳动，从而极大提升分布式协作的整体效率。\n为了让业界能亲身体验，\nTeleAI 已经开源了一个 7B 参数规模的家族式同源模型\n，展示了其在技术落地上的决心。\n有趣的是，TeleAI 给这个模型命名为「\nRuyi\n」，没错，就是「如意金箍棒」的「如意」。它最大 7B，但可以在 3B、4B、5B、6B 之间任意切换，根据实际需求提供智能能力。\n开源地址：\nhttps://github.com/TeleAI-AI-Flow/AI-Flow-Ruyi\nhttps://huggingface.co/TeleAI-AI-Flow/AI-Flow-Ruyi-7B-Preview0704\nhttps://www.modelscope.cn/models/TeleAI-AI-Flow/AI-Flow-Ruyi-7B-Preview0704\n基于连接与交互的智能涌现  如何实现 1+1>2？\n当舞台和演员都已就位，智传网的最终目标是通过连接与交互，催生出超越任何单体能力的「智能涌现」，实现得到 1+1>2 的效果！\n这个理念与诺贝尔物理学奖得主菲利普・安德森（Philip Anderson）在 1972 年提出的「More is Different」（多者异也）思想不谋而合。其背后是业界对于高质量训练数据正快速枯竭的普遍担忧。\nTeleAI 认为，未来的 AI 发展，需要从单纯依赖「数据驱动」转向「连接与交互驱动」。\n具体来说，通过实现模型（包括 LLM、VLM 和扩散模型等不同模型）之间的层级连接与交互，智传网（AI Flow）可整合多种模态和特定领域的专业知识，生成上下文连贯且全局一致的输出，实现超越单个贡献总和的协同能力。\n为此，TeleAI 针对不同类型的任务设计了多种协同模式。\n比如 LLM/VLM 智能体的协同就像「圆桌会议」：想象一个场景，用户提出一个复杂的跨领域问题。智传网（AI Flow）可以同时向部署在不同设备上、分别擅长编码、数学和创意写作的多个 LLM/VLM 智能体发起请求。\n这些智能体各自给出初步答案后，会进入一个「圆桌讨论」环节，相互参考彼此的见解，并对自己的回答进行多轮修正，最终形成一个远比任何单个智能体独立思考更全面、更准确的答案。\n传统的仅服务器范式与设备-服务器协同范式的比较\nTeleAI 也通过大量实验验证了智传网（AI Flow）各组件的有效性，更多详情请参阅技术报告。\n这三大支柱共同发力，使得智传网（AI Flow）不再是一个空想的理论，而是一套具备坚实技术内核、直指产业痛点且路径清晰的系统性解决方案。它为我们揭示了 AI 发展的下一个方向：重要的不再仅仅是计算，更是连接。\nAI 下半场，答案在「连接」里\n从社交媒体的热议，到行业分析报告的「Game Changer」评价，智传网（AI Flow）无疑为我们描绘了一幅激动人心的未来图景。它不仅是 TeleAI 在 AI 时代下出的一步战略好棋，更代表了一种解决当前 AI 领域一大核心矛盾的全新思路。\n回顾全文，智传网（AI Flow）的破解之道是系统性的：它没有执着于打造一个更强的模型或更快的芯片，而是着眼于连接与协同。通过搭建「端-边-云」的层级化舞台，引入能灵活伸缩、高效接力的「家族式同源模型」，并最终催生出「1+1>2」的智能涌现，它成功地在强大的 AI 能力与有限的终端算力之间，架起了一座坚实的桥梁。正如中国电信 CTO、首席科学家，TeleAI 院长李学龙教授说的那样：「\n连接是人工智能发展的关键。\n」我们相信，这也是通往「AI 下半场」的关键。\n人工智能的进一步发展离不开通信和网络基础设施，而这恰恰是运营商特有的优势。实际上，也\n正是因为拥有庞大网络基础设施和深厚云网融合经验，中国电信才能提出并实践这一框架\n。当 AI 不再仅仅是运行在网络之上的应用，而是与网络本身深度融合、成为一种可被调度和编排的基础资源时，一个全新的智能时代便开启了。"}
{"title": "智源RoboBrain 2.0+RoboOS 2.0双发：问鼎评测基准最强具身大脑，刷新跨本体多机协作技术范式", "published_at": "2025-07-14 20:31:00", "url": "https://www.jiqizhixin.com/articles/2025-07-14-10", "content": "近日，智源研究院发布具身大脑\nRoboBrain 2.0 32B\n版本以及跨本体大小脑协同框架\nRoboOS 2.0 单机版\n。\nRoboBrain 2.0，作为集感知、推理与规划于一体面向真实物理环境的 “通用具身大脑”，32B 版本凭借时空认知能力的突破，在\n多项权威具身智能基准上全面刷新纪录\n，此前发布的 7B 版本，具备紧凑高效的模型结构，其轻量化设计\n完美适配边缘设备部署需求\n，能在低资源环境下稳定运行，同时\n相比主流的开闭源模型性能依旧强劲\n。\nRoboOS 2.0 作为\n全球首个具身智能 SaaS 开源框架\n，创新性集成\nMCP 协议与无服务器架构\n，实现轻量化部署，打通智能大脑与异构本体协同通路。同步推出\n单机版产品线及 RoboSkill 技能商店\n，通过深度集成实现机器人技能模块智能匹配与一键适配功能，标准化接口有效消除厂商与硬件适配流程差异。同步推出\n开箱即用镜像\n，支持 \"三行指令\" 极速部署，全面赋能开发者高效构建智能机器人系统。\n具身大脑与跨本体大小脑协同框架双擎联动，将有效推动机器人\n从 “单机智能” 迈向 “群体智能”\n，加速具身智能技术从实验室走向真实场景，形成开放、高效、智能协同的具身智能生态体系。\n1.RoboBrain 2.0突破三大能力瓶颈  模块化架构提升具身复杂推理\n当前主流 AI 模型在应对真实物理环境时，普遍存在三大核心瓶颈：空间理解精度不足、时间依赖建模薄弱、长链推理能力欠缺。RoboBrain 2.0 则在这三大关键能力上实现全面突破，显著提升了对复杂具身任务的理解与执行能力。\n空间理解：\n精确点定位和边界框预测：能够根据复杂指令在图像中定位物体或区域。\n空间关系理解：理解物体之间的相对位置和方向。\n空间推理：支持基于场景图的实时构建和更新，进行复杂的三维空间推理。\n时间建模：\n长期规划：能够进行多步任务规划，支持长期目标的实现。\n闭环交互：支持基于反馈的动态调整，适应动态环境。\n多智能体协作：能够协调多个智能体的行为，完成复杂任务。\n长链推理：\n链式推理：能够进行多步推理，支持复杂任务的逐步解决。\n因果逻辑：能够从复杂指令中提取因果逻辑，并与环境状态对齐。\n决策透明性：能够生成推理过程的详细解释，支持决策的透明性和可解释性。\nRoboBrain能力概览图\nRoboBrain 2.0 采用模块化的编码器 - 解码器架构，为复杂的具身任务实现了\n感知、推理和规划的统一\n。与专注于通用静态视觉问答（VQA）的传统视觉 - 语言模型（VLMs）不同，RoboBrain 2.0 在保持强大通用 VQA 能力的同时，专门针对具身推理任务，如空间感知、时间建模和长链因果推理。该架构将高分辨率图像、多视图输入、视频帧、语言指令和场景图编码为统一的多模态标记序列，以进行全面处理。\nRoboBrain2.0 模型架构图\n2.依托多模态数据集与分阶段训练策略\nRoboBrain 2.0\n刷新性能基准\nRoboBrain 2.0 依托全面且多样化的多模态数据集，融合高分辨率图像、多视角视频序列、场景图、3D 场景数据及复杂自然语言指令，全面赋能机器人在具身环境中的感知、推理与行动能力。该多模态数据集聚焦三大核心领域，为复杂物理场景提供有力支持。\n通用多模态理解\n：整合标准视觉问答、区域级查询、OCR 视觉问答及多轮视觉对话，优化语言表达的多样性与语义一致性，通过丰富的视觉 - 语言交互数据，提升模型对复杂任务的理解与响应能力，适应从简单问答到多轮对话的多样场景。\n空间感知\n：支持高精度物体定位、边界框预测及对象功能性识别，覆盖室内外复杂视觉场景与 3D 空间推理，助力机器人精准解析物体关系、空间属性及场景上下文，应对遮挡、多视角变化等挑战，满足高精度定位与交互需求。\n时间建模\n：通过多模态数据支持长程任务规划、闭环反馈机制及多智能体协作，强化模型在动态环境中的任务分解、动作序列预测及实时交互能力，确保在复杂物理场景中实现连续决策、灵活协作与高效任务执行。RoboBrain 2.0 以卓越的多模态感知、精细的空间推理及强大的长时规划能力，赋能机器人在具身环境中进行交互推理、多智能体协作及高效任务规划，助力复杂物理场景的智能感知与决策。\nRoboBrain 2.0 训练数据集\nRoboBrain 2.0 使用智源自研的大模型训推一体框架 FlagScale 进行大规模分布式训练，采用\n三阶段递进式训练流程\n。\n第一阶段：基础时空学习（Foundational Spatiotemporal Learning）\n在第一阶段，RoboBrain 2.0 专注于构建其在空间感知和时间理解方面的基础能力。模型通过大规模多模态数据集进行训练，这些数据集涵盖了密集标注的图文数据、视频问答以及指代表达理解任务。通过这一阶段的训练，模型能够处理静态图像和视频流，掌握物体的基本空间关系和运动事件，为后续更复杂的任务奠定了坚实的基础。\n第二阶段：具身时空增强（Embodied Spatiotemporal Enhancement）\n在第二阶段，RoboBrain 2.0 通过引入高分辨率多视图图像、第一人称视频数据以及导航和交互任务，进一步增强其在具身任务中的时空建模能力。模型学习处理长序列的时空信息，支持多智能体协调、长期规划和动态环境中的适应性决策。这一阶段的训练使模型能够更好地将历史视觉信息与当前指令相结合，从而在动态交互环境中实现更连贯的长期规划和稳健的场景理解。\n第三阶段：具身情境中的推理链训练（Chain-of-Thought Reasoning in Embodied Contexts）\n在第三阶段，RoboBrain 2.0 通过监督微调和强化微调，进一步提升其在复杂具身任务中的推理能力。模型使用多轮推理示例进行训练，这些示例涵盖了长期任务规划、操作预测、闭环交互、时空理解以及多机器人协作等任务。通过这一阶段的训练，模型能够生成推理链，支持复杂任务的逐步推理和决策，从而在具身情境中实现更高效、更准确的推理和规划能力。\nRoboBrain 2.0 采用 FlagEvalMM 框架，全面验证空间与时间推理能力。\n空间推理\n：在 BLINK（83.95）、CV-Bench（85.75）、Where2Place（73.59）等 9 项基准测试中，RoboBrain-32B/7B-2.0 屡获 SOTA，精准实现物体定位、边界框预测及空间参照，超越 Gemini、GPT-4o 等基线。\n时间推理\n：在多机器人规划（80.33）、Ego-Plan2（57.23）、RoboBench（72.16）中，展现卓越长程规划、闭环反馈及多智能体协作能力，领跑 Qwen2.5-VL、Claude 等模型。\nRoboBrain 2.0-32B 在 BLINK-Spatial、RoboSpatial、RefSpatial-Bench、Where2Place、EgoPlan2 和 Multi-Robot-Plan 等空间与时间推理基准上均取得最佳表现\nRoboBrain 2.0 7B 模型分别以 83.95 分和 85.75 分登顶 BLINK 和 CV-Bench 基准测试。RoboBrain 2.0 32B 模型在 RoboSpatial、RefSpatial-Bench 以及 SAT、Where2Place 和 ShareRobot-Bench 上实现 SOTA 突破\nRoboBrain 2.0 7B 模型在 Multi-Robot Planning 以 81.50 分拔得头筹，RoboBrain 2.0 32B 以 80.33 分紧随其后；RoboBrain 2.0 32B 在 Ego-Plan2（57.23 分）登顶，大幅领先 GPT-4o 等基线；RoboBrain 2.0 7B 模型则在 RoboBench 以 72.16 分夺魁，双模型凭借优异表现刷新性能上限\n3.RoboBrain2.0与RoboOS 2.0双引擎  实现具身群体智能\n依托跨本体大小脑协作框架 RoboOS 2.0 的多本体规划能力，RoboBrain 2.0 已实现多智能体间协作执行任务，支持商超厨房居家等多场景部署。\n跨本体具身大小脑协作框架 RoboOS 2.0 是全球首个基于\n具身智能 SaaS 平台\n、支持无服务器一站式轻量化机器人本体部署的开源框架。同时，RoboOS 2.0 也是全球首个\n支持 MCP\n的跨本体具身大小脑协作框架，旨在构建具身智能领域的\n“应用商店”\n生态。\nRoboOS 2.0 实现了\n大脑云端优化推理部署与小脑技能的免适配注册机制\n，显著降低开发门槛，典型场景下，相关\n代码量仅为传统手动注册方式的 1/10\n。\nRoboOS 2.0 框架（SaaS + MCP 模式）。RoboOS 是面向多机器人协作的 \"大脑 - 小脑\" 分层系统，包含三大核心组件：(a) 基于云计算的具身大脑模型，负责高级认知与多智能体协同；(b) 分布式小脑模块群，专司机器人专项技能执行；(c) 实时共享内存机制，强化环境态势感知能力。\n相较于 1.0，RoboOS 2.0 对端到端推理链路进行了系统级优化，整体性能提升达\n30%\n，基于 FlagScale 端云协同模块，全链路平均响应时延\n低至 3ms\n以下，端云通信效率提升\n27 倍\n。在功能层面，新增了\n多本体时空记忆场景图（Scene Graph）共享机制\n，支持动态环境下的实时感知与建模；同时引入多粒度任务监控模块，实现任务闭环反馈，有效提升机器人任务执行的稳定性与成功率。\nRoboOS 多机协作实现流程包含四个关键阶段：首先通过分层任务分解将复杂任务逐级拆解，随后基于网络拓扑结构进行子任务动态分配，再由分布式智能体集群并行执行各子任务，最后通过实时共享内存机制动态更新环境状态与任务进度。\n基于 RoboOS 2.0 协作框架，可\n充分发挥 RoboBrain 2.0 强大的空间理解、时序规划与闭环推理能力的同时\n，一键下载并部署来自全球开发者创建的相同型号机器人本体的小脑技能，完成大小脑的全链路无缝整合。\nRoboBrain 2.0 可通过像素级空间理解，\n支持下游小脑模型高精度抓取、搬运、放置等操作\n，同时，\n根据实时感知任务执行状态调整执行计划，适应动态环境变化，实现闭环反馈机制。\n4.RoboBrain 2.0与RoboOS 2.0全面开源  携手共建具身智能生态圈\n目前，RoboBrain 2.0 及 RoboOS 2.0 已全面开源，模型权重、训练代码与评测基准全部可用。\nRoboBrain 2.0:\nPage：https://superrobobrain.github.io\nGitHub：https://github.com/FlagOpen/RoboBrain2.0\nArXiv：https://arxiv.org/abs/2507.02029\nCheckpoint-7B：https://huggingface.co/BAAI/RoboBrain2.0-7B\nCheckpoint-32B：https://huggingface.co/BAAI/RoboBrain2.0-32B\nRoboBrain2.0 的 FlagRelease 多芯片镜像：\nhttps://huggingface.co/FlagRelease/RoboBrain2.0-7B-FlagOS\nhttps://huggingface.co/FlagRelease/RoboBrain2.0-32B-FlagOS\nhttps://huggingface.co/FlagRelease/RoboBrain2.0-7B-FlagOS-Ascend\nRoboOS 2.0:\nPage：https://flagopen.github.io/RoboOS\nGitHub：https://github.com/FlagOpen/RoboOS\nGitHub 单机轻量版：https://github.com/FlagOpen/RoboOS/tree/stand-alone\nGitHub 技能商店：https://github.com/FlagOpen/RoboSkill\nArXiv：https://arxiv.org/abs/2505.03673\nRoboBrain 2.0 及 RoboOS 2.0 一经开源，便在全球社交媒体和技术社区引发广泛热议。\n目前，智源研究院已与全球 20 余家机器人企业与顶尖实验室建立战略合作关系，诚邀全球开发者、研究者与产业伙伴加入 RoboBrain 2.0 和 RoboOS 2.0 的开源社区，共筑开放繁荣的具身智能生态。"}
{"title": "ICCV 2025 | 清华&腾讯混元X发现「视觉头」机制：仅5%注意力头负责多模态视觉理解", "published_at": "2025-07-14 20:19:34", "url": "https://www.jiqizhixin.com/articles/2025-07-14-9", "content": "本文的主要作者来自清华大学智能视觉实验室（i-Vision Group）、腾讯混元 X 组。本文的共同第一作者为清华大学自动化系本科生王嘉辉和博士生刘祖炎，本文的通讯作者为清华大学自动化系鲁继文教授。\n多模态大模型通常是在大型预训练语言模型（LLM）的基础上扩展而来。尽管原始的 LLM 并不具备视觉理解能力，但经过多模态训练后，这些模型却能在各类视觉相关任务中展现出强大的表现。\n这引发了我们的思考：在多模态训练过程中，LLM 基座的哪些内部结构，尤其是哪些多头注意力单元，真正承担了对视觉内容的理解？这些注意力头是否存在可识别、可量化的视觉偏好或专业化功能？如果能够识别出这些「视觉头」，不仅有助于揭示多模态大模型内部的「黑箱」机制，也为模型结构优化和资源分配提供了理论依据。\n在本文中，我们聚焦于注意力头的视觉偏好，提出了一种基于 OCR 任务、无需额外训练的方法，系统量化每个注意力头对视觉内容的关注程度。我们发现，只有不到 5% 的注意力头（我们称之为视觉头，Visual Head）在视觉理解任务中起到主导作用，这些头能够有效聚焦并提取图片中的关键信息，而绝大多数注意力头则主要关注文本信息或其他辅助特征。这一「视觉头稀疏性」现象表明，模型的视觉理解能力高度依赖于极少数专门化的注意力头。\n论文标题：SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs\n论文：https://arxiv.org/abs/2506.05344\n代码：https://github.com/CR400AF-A/SparseMM\n项目地址：https://cr400af-a.github.io/SparseMM/\n基于这一发现，我们进一步提出了 SparseMM：一种利用视觉头稀疏性进行 KV-Cache 优化的策略。考虑到多模态大模型输入的视觉 token 远多于文本 token，带来了巨大的显存压力，我们对 KV-Cache 资源进行差异化分配。\n具体地，SparseMM 将总缓存预算划分为三部分：一部分保障所有头的基本局部缓存，一部分按固定比例均匀分配，其余则根据视觉头得分优先分配给视觉头，从而在效率与性能之间取得更优平衡。\n通过在 DocVQA、OCRBench、TextVQA、ChartQA、MMBench、GQA 等主流多模态基准上的广泛评测，SparseMM 相较于 SnapKV、AdaKV 等方法取得了更好的性能和效率的平衡。效率评估测试中实现了最高 1.87× 的解码阶段加速并降低了 52% 的峰值内存。此外，在极端缓存预算下，性能下降幅度更小，充分验证了基于视觉头的 KV-Cache 分配策略在效率-性能权衡上的优越性。\n介绍\n多模态大模型通过引入视觉编码器模块，使得原本不具备视觉能力的 LLM 能够在图文问答、文档理解等多种场景下表现出色。但是模型内部究竟是如何实现这一跨模态迁移的，仍然是一个「黑箱」问题。我们认为，在多模态大模型训练的过程中，部分注意力头逐渐特化为了「视觉头」，专门负责视觉信息的理解与交互。\n在本文中，我们提出了一种\n基于 OCR 任务量化并识别视觉头（Visual Head）\n的方法，并基于此提出了\nSparseMM\n——一种新颖的多模态模型推理加速方法。通过对视觉头的深入分析，我们发现视觉头在多模态大模型中占比很小。\n也就是说，只有一小部分注意力头真正承担了对视觉内容进行深度理解并将其有效融入语言表征的核心任务，而大多数注意力头更多地关注语言信息，或仅局限于局部上下文建模，对图像内容的理解作用有限。\n基于此，我们采用了一种\n注意力头级别的缓存分配机制\n，对更关注视觉内容的注意力头分配更多的缓存预算，以最大程度的保留视觉信息；对于不关注视觉内容的注意力头则分配较少的缓存预算，使它们关注最近邻的信息即可，从而实现了性能和速度的更优均衡。\n图 1：SparseMM 整体概览\n方法概览\n我们的方法主要分为两部分：首先通过 OCR 任务定位视觉头，然后为不同的注意力头分配不同的缓存预算。\n基于 OCR 的视觉头定位方法\n图 2：SparseMM 基于 OCR 任务定位视觉头的方法示意图\n为了深入探究多模态大模型在处理视觉内容时的注意力机制，我们提出了一种基于 OCR 任务的分析方法，并据此定义了「视觉得分」，用于量化模型在视觉内容上的注意力表现。基于视觉得分，本文能够有效定位并分析模型内部对视觉内容高度敏感的注意力头。\n具体而言，在给定一个 OCR 任务的图片输入时，多模态大模型需要根据图片内容生成并输出图片中的文字信息。对于每一个由模型输出的 token\n，首先利用 OCR 任务的标注信息，即「(text, bbox)」对，明确该字符在图像中的空间位置。\n通过这一标注，可以将每个字符与其在图片中的具体区域一一对应。接下来，按照多模态大模型对输入图片的分块或 patch 划分方式，进一步确定每个字符对应的视觉区域所映射到的视觉 token，并精确定位这些视觉令牌在整个输入序列中的具体位置。\n在此基础上，我们对多模态大模型内部所有注意力头进行遍历。对于任意一个注意力头，我们分析其注意力得分矩阵。考虑当前字符 token\n对前序所有输入 token 的注意力得分，若得分最高的 token 恰好属于该字符在图像中对应区域的视觉 token，则认为该注意力头在该位置成功「命中」了对应的视觉内容。每当发生一次「命中」，便为该注意力头累计一次视觉得分。通过统计和归一化所有字符令牌的命中情况，最终可以量化每个注意力头对视觉内容的关注程度，从而揭示模型在视觉信息处理过程中的内部机制。\n基于视觉头的 KV-Cache 压缩策略\n在完成视觉头的定位之后，我们进一步提出了一种基于视觉头的 KV-Cache 分配与压缩策略。传统的 KV-Cache 机制为所有注意力头和所有位置的 token 分配等量的缓存空间，这种方式虽然简单，但在处理高分辨率图像时显得极为低效：大量不关注视觉内容的注意力头被迫缓存完整视觉 token，造成了显著的资源浪费。\n为了解决这一问题，SparseMM 根据视觉得分设计了一个三部分的缓存分配机制：\nLocal Window Cache： 为所有注意力头分配固定窗口大小的缓存，只保留最近的若干个 token，确保基本的局部上下文建模能力；\nUniform-Based Cache： 在所有注意力头之间均匀分配一部分缓存预算，用于保底防止头部信息过度丢失；\nScore-Preferred Cache： 将剩余的大部分缓存资源按照视觉头在前一阶段中的视觉得分按比例分配，使得关键的视觉头能够尽可能保留更多的历史视觉 token，从而提升模型对图像语义的保持能力。\n这种差异化的缓存压缩策略在不显著牺牲模型性能的情况下，显著减少了整体 KV-Cache 的内存使用。尤其在视觉 token 占比较高的输入场景中，SparseMM 能够更合理地分配资源，把计算和存储集中在真正重要的视觉内容上。\n图 3：SparseMM 基于视觉头的缓存压缩方法\n实验结果\n在 OCR-rich 的多模态数据集上的结果\n在 OCR-rich 的数据集上（如 DocVQA、OCRBench 和 TextVQA），SparseMM 展现出显著的性能优势，充分验证了其视觉头识别策略的有效性。例如在 DocVQA 中，当键值缓存预算仅为输入长度的 10% 左右时，LLaVA-NeXT-Vicuna-7B 与 Qwen2-VL-7B-Instruct 等模型仍能保持与全缓存配置几乎一致的性能，而现有方法则普遍出现明显精度下降，差距在低预算下进一步扩大，突出体现了视觉头选择的准确性和关键性。TextVQA 中的实验同样验证了 SparseMM 的优势，多个模型在低至 5% 至 10% 缓存预算的条件下依然保持优异性能，显著优于 AdaKV、SnapKV 等方法。这些结果表明，SparseMM 尤其适用于文字密集、图文关联紧密的视觉任务，在处理高分辨率输入与稀疏文本分布场景中具备显著的推理效率与性能保持能力。\n通用多模态任务上的分析\n尽管本文的视觉头识别方法基于 OCR 任务构建，但是为了进一步验证其在更广泛视觉场景中的适用性与泛化能力，我们在多个通用视觉任务基准（如 MMBench、GQA 和 VQAv2）上对该方法进行了系统性评估。\n实验结果显示，本文方法在通用视觉任务中依然表现出极强的鲁棒性与泛化能力。即便在非常受限的缓存预算的条件下，Qwen2-VL-7B-Instruct 模型在 MMBench 上仍能维持与全缓存模型几乎一致的性能；在 GQA 和 VQAv2 等具备复杂视觉推理能力要求的任务上，性能下降幅度也始终控制在 1% 以内，显著优于现有压缩方法。这些结果表明，尽管视觉头的识别基于 OCR 场景完成，其关注的视觉区域和注意力机制却具有高度的通用性，能够在各类视觉理解任务中稳定发挥作用，为通用多模态模型的推理加速与缓存优化提供了一种高效、可靠且可推广的解决方案。\n推理速度评估\n本文在不同输入长度（2K 至 32K）场景下评估了 SparseMM 的计算效率，结果显示该方法在提升推理速度和降低显存占用方面均取得显著提升。在 32K 输入下，LLaVA-NeXT-Vicuna-7B 和 Qwen2-VL-7B-Instruct 的推理速度分别提升至 1.87× 和 1.60×，而峰值显存占用分别减少约 15GB 和 2GB，表现出良好的扩展性与适应性。这充分说明 SparseMM 在高分辨率图像或长上下文任务中，能够有效降低推理开销，提升多模态大模型的部署效率与实用性。\n可视化视觉头\n我们可视化了 LLaVA-NeXT-Vicuna-7B 中识别到的一些视觉头和非视觉头，可以看出视觉头能准确的定位到图中的物体或文字，而非视觉头往往不关注图像信息或者关注到错误的区域，这直观地体现了视觉头和非视觉头的差异性。\n总结\n我们提出了 SparseMM，这是一种基于视觉头的 KV-Cache 缓存压缩方法。我们通过在 OCR 任务中精确识别出对视觉信息最敏感的注意力头，并据此设计差异化的缓存分配策略，在保证模型性能的同时显著降低了推理阶段的计算和内存开销。\n实验结果表明，SparseMM 在多个视觉语言任务中均展现出卓越的准确性保持能力、优异的计算效率以及强大的泛化性，特别是在高分辨率图像和长上下文输入场景下具有显著优势。SparseMM 为多模态大模型的高效推理与实际部署提供了新的解决思路，我们也希望这项工作能启发未来更多对多模态大模型推理加速的研究。"}
{"title": "夜场预告 | WAIC UP!之夜：不是大会延长时，而是另一种打开AI的方式", "published_at": "2025-07-14 14:23:17", "url": "https://www.jiqizhixin.com/articles/2025-07-14-8", "content": "Hello:\n世界人工智能大会（WAIC）推出首份刊物\n《WAIC UP!》\n，一部「AI时代进化指南」。\nWAIC UP! WAKE UP MORE！\n我们邀请全球AI及跨领域的先锋力量，共同释放思考的力量与智慧的主张，旨在唤醒更多人，探究关乎技术跃迁、自我边界和未来文明的无限可能。\n准备好！释放你的思考，觉醒你的行动，与我们一同探究那些尚未显现的可能性，勾勒以人为中心的未来智慧文明全景图！\n《WAIC UP!》\n即刻解锁你的「AI时代进化指南」"}
{"title": "智能科学实验室加速未来科学发现，首版仿真智驱实验室LabUtopia发布", "published_at": "2025-07-14 14:04:00", "url": "https://www.jiqizhixin.com/articles/2025-07-14-7", "content": "编辑丨ScienceAI\n打破家庭场景「内卷」，机器人不仅能下厨，还要上实验台！LabUtopia把「智能科学实验室」搬进虚拟仿真器。\n智能科学实验室新范式\n当前「AI 科学家」侧重虚拟推理、自动化实验室侧重机械执行，二者各擅其长却难以贯通。要真正实现全流程闭环、自主进化的科学发现，必须把认知大模型、智能体工作流和具身机器人三大技术栈深度耦合，构建智能科学实验室（Intelligent Science Laboratory，ISL）新范式。\n由上海人工智能实验室、中国科学技术大学、牛津大学、新南威尔士大学、洛桑联邦理工、北京大学、中科院、南京大学等十余所高校和研究机构的研究者联合撰写的立场论文呼吁：\n「智能科学实验室」（Intelligent Science Laboratory, ISL）\n将成为未来科学发现的关键范式，并且其核心在于深度融合认知智能与具身智能，实现从假设生成到实验验证的闭环式科学探索自动化。\n期待\n这能成为通向 AI 驱动科学的新起点！\n研究团队以「基座模型-智能体-具身执行」三层闭环架构为核心，配套四级渐进式路线图，将认知大模型、智能体工作流和具身机器人深度耦合，引导实验室从基础自动化稳步进化到完全自主的智驱科研新范式。\n团队呼吁全球研究者、开发者与产业伙伴加入 ISL 生态，共同加速科学发现的未来。\nLabUtopia\n在具身智能模拟平台「卷爆家庭日常」的今天，从厨房炒菜到客厅扫地，AI Agent 似乎无所不能。然而，真正挑战复杂物理交互和长链条规划的科研实验室场景，却始终缺位。\n如今，这一空白被正式填补——LabUtopia 正式发布！\n来自上海人工智能实验室、北京大学、牛津大学、中国科学技术大学等机构的联合团队推出 LabUtopia：全球首个支持化学反应建模 + 流体物理模拟 + 多步实验流程规划与评估的高保真科学实验具身智能平台。\n它不仅支持化学反应的建模、流体-刚体-软体的交互模拟，还首次提出「五级任务评估体系」，让机器人不再只停留于「夹起杯子」，而是能执行复杂、长周期的科研实验流程，一步步推动机器人迈入「科研自动化」新时代。\n这不仅是具身模拟器的一小步，更是通往「AI 化科研发现」的关键跃迁！\n为什么科研实验室是下一个「具身智能战场」？\n相较于日常生活场景，科研实验室更具挑战性，也更具「科学智能」价值：\n物理挑战升级：液体倾倒、溶液混合、材料加热、化学反应观察等，需要高保真的流体+刚体+化学过程建模；\n任务链更长更敏感：\n精细操作（如玻璃棒搅拌液体），\n从「抓取试管」到「完成一整套滴定实验」，流程长、误差敏感、需稳定执行；\n智能能力更高：\n从「抓取试管」到「完成产物制备」，任务往往由多个步骤组成，需高层级规划与容错机制：\n不仅要操作，还要识别材料状态变化，具备感知-推理-控制的整合能力。\nLabUtopia 正是在这一背景下诞生，它将科研实验室真正带入具身智能模拟的版图，提供全面覆盖、真实建模、层级式评估的一体化解决方案。\n平台介绍\n科学实验室仿真平台需要哪些关键技术来支撑对于科学实验的模拟执行？\n通过分析，研究团队确定了一个完整的、可执行的科学仿真平台所必需的三项核心模块：高保真仿真引擎、真实科研实验室构建器以及科研任务评估体系。\nLabsim的示意图\nLabSim：高保真仿真引擎\n基于 NVIDIA Isaac Sim 深度定制的物理引擎，LabSim 不仅支持刚体与软体的高精度交互，还首次整合了流体动力学模拟与动态化学反应可视化能力。\nLabScene：真实科研实验室构建器\n传统模拟器中的「房间生成」在实验室任务面前失效。LabScene 构建了一个包含 200+ 仪器/器具的可组合实验空间，支持程序生成 + 物理可行性检查 + 专家验证三重约束生成，确保每一个场景「既复杂又真实」。\nLabBench：五级任务评估体系\n科学实验任务不像厨房炒菜，是标准化、多阶段、易出错、可量化的任务链。LabBench 引入业内首个五级任务分层体系，从原子操作到跨区域长链任务，具体包括原子操作、短程操作、通用短程操作、长程操作以及移动长程操作，提供逐步进阶、可控难度、跨算法对比的系统化评估平台。\nLabBench示意图\n实验：现有算法在科学场景面前「落败」！\n我们系统评估了两个主流模仿学习算法（ACT 与 Diffusion Policy）在 LabUtopia 上的表现，发现：\n✅ 基础操作（如 Press, Stir）ACT 成功率可达 90%+\n⚠️ 多步任务（如 Transport Beaker、Heater Beaker）开始出现显著失误\n❌ 长链任务（如 Clean Beaker，7 步）整体成功率跌至个位数，DP 几乎全军覆没！\n这表明：现有通用具身策略在科学实验中的泛化性、稳定性远未达标，而 LabUtopia 正提供了一个标准、开放的评测平台，驱动下一个具身智能研究浪潮。\n论文链接：https://arxiv.org/abs/2506.19613  and https://arxiv.org/abs/2505.22634\nBenchmark以及主页：https://rui-li023.github.io/labutopia-site/\n人工智能（AI）正在深刻变革科学研究的范式，显著提升科学发现的效率、精确性和创新性。AI for Science团队，探索人工智能驱动科学问题的研究范式，围绕「通专融合AGI」之路，搭建自然科学从微观到宏观跨学科的数据、算法和科学发现平台，推动AI for Science从「工具的革命」迈向「革命性的工具」，并加速人工智能在各学科领域的创新与落地，赋能各行业发展。"}
{"title": "Windsurf交易内幕疯传：24亿美元被瓜分，背刺数百员工？", "published_at": "2025-07-14 13:25:09", "url": "https://www.jiqizhixin.com/articles/2025-07-14-6", "content": "这个周末，大家都在看 OpenAI 的热闹。\n起因是谷歌 DeepMind 截胡了 OpenAI 原本打算收购\nWindsurf 的计划\n。此前 OpenAI 就以 30 亿美元收购这家初创公司一事展开了长达数月的谈判，没想到，还是被 DeepMind 抢先了。\n据了解，谷歌将向 Windsurf 支付 24 亿美元，把包括 Windsurf CEO Varun Mohan、联合创始人 Douglas Chen 在内的核心团队一整个打包带走。谷歌明确表示不会收购该公司股份，Windsurf 将继续作为一家独立公司运营。\n约250名员工中的大部分将继续留任，专注于为大型企业开发编程工具。\nWindsurf 创始人 Varun Mohan（左）和 Douglas Chen（右）。\n一笔高达 24 亿美元的交易，本应是初创公司 Windsurf 的高光时刻，但新披露的细节却揭示了残酷的一面。\n这笔巨款的大部分将直接流向公司的创始人、部分被谷歌选中的工程师以及早期的投资者。据消息人士透露，创始人和这个被选中的小团队将从中大赚一笔，而去年以 12.5 亿美元估值支持公司的优先股股东也能获得回报。\n这是一次典型的「反向人才收购」（Reverse Acqui-hire）。此交易并非传统意义上的全盘收购，其核心是人才与技术的精准剥离：\n人才收购 (Acqui-hire)： 谷歌的目标是 Windsurf 的顶尖团队。交易后，其创始人及核心工程师将并入谷歌 DeepMind。\n技术授权 (Licensing)： 谷歌通过非排他性技术许可，获得了 Windsurf 的创新成果，以强化其在 AI 编码市场的竞争力。\n此「反向」操作的精髓在于，谷歌只吸纳最精华的人才与技术，而将 Windsurf 的公司「空壳」连同部分资产与剩余员工一并留下。\n这些被留下的员工，无论过往股权如何，都无法从这笔交易中直接获益。 他们得到的是：一个失去了核心技术（已授权给谷歌）、前途未卜的新公司。\n新 Windsurf 公司面临的未来异常严峻。它不仅要与转投谷歌的前创始人和核心团队竞争，还要面对 Cursor、Anthropic 等代码生成领域的强敌。舆论普遍悲观，认为失去核心优势的 Windsurf 最终可能价值归零。\n这笔交易打破了初创公司的隐性契约 —— 员工曾以低薪和高风险换取未来的股权回报。如今，创始人和投资者却设计了一套利己方案，将大部分利益收入囊中，置员工于不利境地。\n此事件被与谷歌收购 CharacterAI 相提并论，但 Windsurf 的处境更糟，因为谷歌将直接在其核心业务上展开竞争。\n外界猜测，这或许是 Windsurf 领导层在巨大竞争压力下为自己安排的退路。不过也有观点认为，这些创始人的「背刺」行为，会让谷歌内部的员工对他们的人品和领导力产生质疑。\n与 OpenAI 的交易为何告吹？\nOpenAI 原本计划以 30 亿美元收购 Windsurf，但该计划最终因其最大投资者微软的介入而失败。核心矛盾在于知识产权的归属问题。\n根据 OpenAI 与微软的协议，微软有权获取 OpenAI 所有的知识产权。然而，Windsurf 的领导层不愿意将其核心技术与 AI 编码领域的直接竞争对手（即微软支持下的 OpenAI）分享。\n当 OpenAI 无法就此条款向微软争取到例外时，其与 Windsurf 的独家收购谈判期限最终失效，这为谷歌的迅速介入创造了机会。\n大家反应如何？\n对于收购这件事，大家基本都持有同样的看法，为 Windsurf 员工抱不平。\n「一旦创始人用名誉换取金钱，他们的名誉就永远无法挽回了。他们将永远无法成就一番大事业，因为没有人再信任他们。之后的生活将会变得无趣。」\n「如果关于 Windsurf 的传闻属实，那将是我听说过的高管团队最肮脏、最卑劣的操作之一。这些人应该被硅谷彻底封杀，而不是得到奖赏。实在难以想象，他们竟能如此恶心地背刺自己的员工。」\n「虽然不了解全部细节 —— 但如果 Windsurf 的创始人真的拿着巨额酬金一走了之，把员工晾在一边，那这退场方式也太吃相难看了。创始人引领公司方向，团队才是真正的建造者。真正的领导者永远把团队放在首位。」\n不过也有人说这是一场沟通危机（谷歌的介入阻碍了创始人与团队的沟通），最终被留下的团队在（资产）分配完成后将获得公平的待遇。\n大家怎么看，欢迎评论区留言。\n参考链接：\nhttps://x.com/jordihays/status/1944200891944644997\nhttps://quasa.io/media/google-s-2-4-billion-windsurf-deal-a-strategic-win-or-a-blow-to-startup-ideals"}
{"title": "用动作分块突破RL极限，伯克利引入模仿学习，超越离线/在线SOTA", "published_at": "2025-07-14 13:23:16", "url": "https://www.jiqizhixin.com/articles/2025-07-14-5", "content": "如今，强化学习（Reinforcement Learning，RL）在多个领域已取得显著成果。\n在实际应用中，具有长时间跨度和稀疏奖励特征的任务非常常见，而强化学习方法在这类任务中的表现仍难令人满意。\n传统强化学习方法在此类任务中的探索能力常常不足，因为只有在执行一系列较长的动作序列后才能获得奖励，这导致合理时间内找到有效策略变得极其困难。\n假如将模仿学习（Imitation Learning, IL）的思路引入强化学习方法，能否改善这一情况呢？\n模仿学习通过观察专家的行为并模仿其策略来学习，通常用于强化学习的早期阶段，尤其是在状态空间和动作空间巨大且难以设计奖励函数的场景。\n近年来，模仿学习不仅在传统的强化学习中取得了进展，也开始对大语言模型（LLM）产生一定影响。近日，加州大学伯克利分校的研究者提出了一种名为\nQ-chunking\n的方法，该方法将动作分块（action chunking）—— 一种在模仿学习中取得成功的技术 —— 引入到基于时序差分（Temporal Difference, TD）的强化学习中。\n该方法主要解决两个核心问题：一是通过时间上连贯的动作序列提升探索效率；二是在避免传统 n 步回报引入偏差的前提下，实现更快速的值传播。\n论文标题：Reinforcement Learning with Action Chunking\n论文地址：https://www.alphaxiv.org/overview/2507.07969v1\n代码地址：https://github.com/ColinQiyangLi/qc\n如下图 1 左所示，Q-chunking（1）使用动作分块来实现快速的价值回传，（2）通过时间连贯的动作进行有效探索。图 1 右中，本文方法首先在离线数据集上进行 100 万步的预训练（灰色部分），然后使用在线数据更新，再进行另外 100 万步的训练（白色部分）。\n问题表述与研究动机\nQ-chunking 旨在解决标准强化学习方法在复杂操作任务中存在的关键局限性。\n在传统强化学习中，智能体在每一个时间步上逐一选择动作，这常常导致探索策略效率低下，表现为抖动、时间不连贯的动作序列。这一问题在\n稀疏奖励环境\n中尤为严重 —— 在此类环境中，智能体必须执行较长的、协调一致的动作序列才能获得有效反馈。\n研究者提出了一个关键见解：尽管马尔可夫决策过程中的最优策略本质上是马尔可夫性的，但\n探索过程却可以从非马尔可夫性、时间上扩展的动作中显著受益。\n这一观察促使他们将「动作分块」这一原本主要用于模仿学习的策略引入到时序差分学习中。\n该方法特别面向\n离线到在线\n的强化学习场景（offline-to-online RL），即智能体先从预先收集的数据集中进行学习，再通过在线交互进行微调。这一设定在机器人应用中尤为重要，因为在线数据采集成本高且可能存在安全风险。\n方法概览\nQ-chunking 将标准的 Q-learning 扩展至\n时间扩展的动作空间\n，使策略不再仅预测单一步骤的动作，而是预测连续 h 步的动作序列。该方法主要包含两个核心组成部分：\n扩展动作空间学习\n传统方法学习的是针对单步动作的策略 π(aₜ | sₜ) 和 Q 函数 Q (sₜ, aₜ)，而 Q-chunking 学习的是：\n* 块状策略（Chunked Policy）：π_ψ(aₜ:ₜ₊ₕ | sₜ)\n* 块状 Q 函数（Chunked Q-function）：Q_θ(sₜ, aₜ:ₜ₊ₕ)\n核心创新体现在时间差分损失（TD loss）的构造上。块状 Q 函数的更新方式如下：\n该形式实现了\n无偏的 h 步的值传播\n，因为 Q 函数以整个动作序列作为输入，从而消除了传统 n 步回报中存在的离策略偏差（off-policy bias）。\n行为约束\n为了保证时间上的连贯性探索，并有效利用离线数据，Q-chunking 在扩展动作空间中对学习到的策略施加了行为约束，使其保持接近离线数据分布。该约束表达如下：\n其中，D 表示一种距离度量方法，π_β 是来自离线数据集的行为策略。\n算法实现\n研究者展示了Q-chunking框架的两种实现方式：\nQC（带有隐式 KL 约束的 Q-chunking）\n该分支通过「从 N 个中选择最优」（best-of-N）的采样策略，隐式地施加 KL 散度约束。其方法如下：\n1. 在离线数据上训练一个流匹配行为策略 f_ξ(・|s)\n2. 对于每个状态，从该策略中采样 N 个动作序列（action chunks）\n3. 选择具有最大 Q 值的动作序列：a* = arg max_i Q (s, a_i)\n4. 使用该动作序列进行环境交互与 TD 更新\nQC-FQL（带有 2-Wasserstein 距离约束的 Q-chunking）\n该实现基于 FQL（Flow Q-learning）框架：\n1. 保持一个独立的噪声条件策略 μ_ψ(s, z)\n2. 训练该策略以最大化 Q 值，并通过正则项使其靠近行为策略\n3. 使用一种蒸馏损失函数，对平方的 2-Wasserstein 距离进行上界估计\n4. 引入正则化参数 α 来控制约束强度\n实验设置及结果\n关于实验环境和数据集，研究者首先考虑 6 个稀疏奖励的机器人操作任务域，任务难度各不相同，包括如下：\n来自 OGBench 基准的 5 个任务域：scene-sparse、puzzle-3x3-sparse，以及 cube-double、cube-triple 和 cube-quadruple，每个任务域包含 5 个任务；来自 robomimic 基准中的 3 个任务。\n对于 OGBench，研究者使用默认的「play-style」数据集，唯独在 cube-quadruple 任务中，使用了一个规模为 1 亿大小的数据集。\n关于基线方法比较，研究者主要使用了以加速「价值回传」为目标的已有方法，以及此前表现最好的「离线到在线」强化学习方法，\n包括 BFN（best-of-N）、FQL、BFN-n / FQL-n 以及 LPD、RLPD-AC\n。\n下图 3 中展示了 Q-chunking 与基线方法在 5 个 OGBench 任务域上的整体性能表现，下图 4 中展示了在 3 个 robomimic 任务上的单独性能表现。其中在离线阶段（图中为灰色），QC 表现出具有竞争力的性能，通常可以比肩甚至有时超越了以往最优方法。而在在线阶段（图中为白色），QC 表现出极高的样本效率，尤其是在 2 个最难的 OGBench 任务域（cube-triple 和 quadruple）中，其性能远超以往所有方法（特别是 cube-quadruple 任务）。\n下图 5 为消融实验，比较了 QC 与其变体 QC-FQL、以及 2 种 n 步回报的基线方法（BFN-n 和 FQL-n）。这些 n 步回报基线方法没有利用时间扩展的 critic 或 policy，因此其性能显著低于 QC 和 QC-FQL。实际上，它们的表现甚至常常不如 1 步回报的基线方法 BFN 和 FQL，这进一步突显了在时间扩展动作空间中进行学习的重要性。\n接下来探讨的问题是：为什么动作分块有助于探索？研究者在前文提出了一个假设：动作分块策略能够\n生成在时间上更连贯的动作\n，从而带来更好的状态覆盖和探索效果。\n为了进行实证，他们首先可视化了训练早期 QC 与 BFN 的末端执行器运动轨迹，具体如下图 7 所示。可以看到，BFN 的轨迹中存在大量停顿（在图像中心区域形成了一个大而密集的簇），特别是在末端执行器下压准备抓取方块时。而 QC 的轨迹中则明显停顿较少（形成的簇更少且更浅），并且其在末端执行器空间中的状态覆盖更加多样化。\n为了对动作的时间连贯性进行定量评估，研究者在训练过程中每 5 个时间步记录一次 3D 末端执行器位置，并计算相邻两次位置差向量的平均 L2 范数。如果存在较多停顿或抖动动作，该平均范数会变得较小，因此可以作为衡量动作时间连贯性的有效指标。\n正如图 7（右）所示，在整个训练过程中，QC 的动作时间连贯性明显高于 BFN。这一发现表明，QC 能够提高动作的时间连贯性，从而解释了其更高的样本效率。\n更多细节内容请参考原论文。"}
{"title": "ACL 2025｜自我怀疑还是自我纠正？清华团队揭示LLMs反思技术的暗面", "published_at": "2025-07-14 13:19:07", "url": "https://www.jiqizhixin.com/articles/2025-07-14-4", "content": "本文第一作者是张清杰，清华大学博士生，研究方向是大语言模型异常行为和可解释性；本文通讯作者是清华大学邱寒副教授；其他合作者来自南洋理工大学和蚂蚁集团。\n反思技术因其简单性和有效性受到了广泛的研究和应用，具体表现为在大语言模型遇到障碍或困难时，提示其“再想一下”，可以显著提升性能 [1]。然而，2024 年谷歌 DeepMind 的研究人员在一项研究中指出，大模型其实分不清对与错，如果不是仅仅提示模型反思那些它回答错误的问题，这样的提示策略反而可能让模型更倾向于把回答正确的答案改错 [2]。\n基于此，来自清华大学、南洋理工大学和蚂蚁集团的研究人员进一步设想，如果模型没有外部的认知控制（避免使用说服语和误导性质的词语），仅通过提示其 「思考后再回答」，其表现会如何呢？结果发现，模型的表现仍然不尽如人意。如下动画所示，OpenAI 于 2025 年 4 月 16 日最新推出的能在 AIME 数学竞赛上取得 99.5% pass@1 成绩的\n推理模型 ChatGPT o4-mini-high 甚至在简单的事实问题上 「地球是不是平的？」 也会出错。\n图 1: 反思技术会导致 OpenAI 先进的推理模型 o4-mini-high 在简单事实问题 「Is Earth flat?」 上出错。尽管推理过程认为地球不是平的，模型最终答案仍然出错。（实验时间：2025 年 7 月 4 日）\n因此，本研究设计三种解释方法，深入剖析了没有外部认知控制的反思技术（Intrinsic self-correction，下文中简称为反思技术）在开源和闭源的 LLMs、四种任务上失败的原因，并且提出轻量级的缓解方案（问题重复，少样本微调），为反思技术的可解释性研究奠定基础。\n论文标题：Understanding the Dark Side of LLMs’ Intrinsic Self-Correction\n项目网站：https://x-isc.info/\n论文发表：ACL 2025 main（主会）已接受，审稿人提名 「Best paper: Maybe」\n反思技术的失败情况\n这项研究首先系统性评测了反思技术在多种 LLMs，多种任务中的失败情况。\nLLMs：ChatGPT (o1-preview, o1-mini, 4o, 3.5-turbo), Llama (3.1-8B, 3-8B, 2-7B), DeepSeek (R1, V3)\n任务：Yes/No questions, Decision making, Reasoning, Programming\n如下表所示，\n反思技术在包括简单事实问答任务和复杂推理任务的多种任务中都会失败，甚至比成功的案例多。\n对于更先进的模型，反思失败有减少但没有解决，甚至在部分任务中更加严重。例如，o1-mini 在 Decision making 任务上的反思失败率（将初始正确答案改错的概率）高于 4o 和 3.5-turbo；Llama-3.1-8B 在 Yes/No questions 任务上的反思失败率高于 Llama-2-7B。\n表 1: 反思技术在多个 LLMs，多种任务中的失败情况。（实验时间：2025 年 2 月 15 日）注：更多例子参见论文网站：https://x-isc.info\n研究团队近期对一些最新的 ChatGPT 模型（4.5，4.1，o4-mini，o3）也进行了评测。如下表所示，反思失败情况同样严重。\n表 2: 反思技术在最新的 ChatGPT 模型上也容易失败。（实验时间：2025 年 7 月 4 日）\n原因一：内部答案波动 —— 自我怀疑？\n为了解释反思失败的原因，本研究从简单事实问题入手，观测了 LLMs 在回复时的答案波动情况。如下图所示，研究团队观察到在多轮问答任务上，「你确定吗？请思考后再回答」 的提示语会让\nLLMs 反复更改答案\n。例如在 10 轮对话中，GPT-3.5-turbo 甚至对于 81.3% 的问题更改答案超过 6 次。\n图 2: LLMs 在多轮对话中会频繁更改答案。（实验时间：2025 年 2 月 15 日）\n这一现象意味着 LLMs 也许对于自己的答案是不自信的。因此，研究团队利用探针方法 [3] 逐层分析了 Llama-3-8B 对于正确、错误答案的置信度。如下图所示，与初始回复相比，\n反思技术会造成 LLMs 内部答案的波动，表现出 「自我怀疑」 的倾向，最终可能导致回答出错\n；并且，研究发现提示模型 「你确定吗？」 的内部状态表现与告诉模型 「你的回答错了」 相似。因此，内部答案波动是反思技术失败的原因。\n图 3: 反思技术会导致 LLMs 的内部答案波动（左图）。而右图显示：对 Llama3-8B 模型而言，提示 「你确定吗？」 对模型的影响与提示 「你的回答错了」 非常相似。\n原因二：提示语偏差 —— 过度关注反思指令\n对于内部状态不可知的黑盒模型，研究团队进一步从提示语层面分析了词元对 LLMs 输出答案的贡献度。如下图所示，\nLLMs 在反思失败时会过度关注提示语 「你确定吗？想一想再回答。」，而忽略问题本身\n；当反思失败时，LLMs 在 76.1% 的情况下会更关注反思指令，而当坚持正确答案时，LLMs 对反思指令和问题本身的关注度非常相近，分别为 50.8% 和 49.2%。这一现象意味着 LLMs 对提示语的理解往往与人类的期望存在偏差，从而导致任务失败。\n图 4: 反思技术会导致 LLMs 过度关注反思指令而忽略问题本身。绿色 / 黄色表示 LLMs 关注多 / 少的词元。\n原因三：认知偏差 —— 像人一样犯错\n对于复杂任务，研究团队进一步分析了 LLMs 的推理过程，发现 LLMs 会像人一样犯错。如下图所示，反思技术会让 LLMs 在 Decision-making 任务中生成过量的 「think」 指令，导致过度思考策略而停滞不前。基于这一发现，\n研究团队进一步应用认知科学理论将 LLMs 的反思失败总结成三种认知偏差模式：\n过度思考\n：过度制定策略而不采取行动\n认知过载\n：在长文本的反思中忽略关键信息\n完美主义偏差\n：为了追求高效性而忽略环境限制\n图 5: 反思技术会导致 LLMs 在推理过程中出现认知偏差。\n缓解策略\n基于反思失败的原因，研究团队进一步设计了两种简单有效的缓解策略：\n问题重复：基于原因二中 LLMs 更关注反思指令而忽略初始问题的发现，研究团队在反思提示语的最后附上初始问题以引导 LLMs 维持对初始问题的关注。\n少样本微调：基于原因一中反思引起 LLMs 内部状态的异常波动，以及原因三中 LLMs 在推理过程中的认知偏差，研究团队认为反思失败是一种异常行为 [4]，并非知识匮乏。因此，不引入知识的少样本（4-10 个样本）微调可纠正反思失败的异常行为。\n实验结果如下表所示，两种策略皆可有效缓解反思失败，少样本微调的效果更好；并且，\n由于反思失败是一种异常行为而非知识匮乏\n，在简单任务上的少样本微调效果可以泛化到复杂任务上。\n表 3：问题重复和少样本微调可有效缓解反思失败。（实验时间：2025 年 2 月 15 日）\n总结\n该研究系统性评测了 LLMs 反思技术的失败，发现这种现象在多个 LLMs、多种任务上广泛存在，甚至先进的推理模型（ChatGPT o4-mini-high）在基本事实问题（「Is Earth flat?」）上也会出错。进而，研究团队揭示了反思失败的三种原因：内部答案波动，提示语偏差，认知偏差。基于这些原因，研究团队设计了两种简单有效的缓解反思失败的策略：问题重复和少样本微调。\n反思技术究竟引向自我纠正还是自我怀疑\n，这仍然是一个悬而未决的问题。\n参考文献\n[1] Reflexion: Language agents with verbal reinforcement learning, NIPS 2023.\n[2] Large language models cannot self-correct reasoning yet, ICLR 2024.\n[3] Eliciting latent predictions from transformers with the tuned lens, arXiv 2023.\n[4] https://openai.com/index/chain-of-thought-monitoring/"}
{"title": "ICCV 2025满分论文：一个模型实现空间理解与主动探索大统一", "published_at": "2025-07-14 11:18:54", "url": "https://www.jiqizhixin.com/articles/2025-07-14-3", "content": "本论文核心团队来自北京通用人工智能研究院机器学习实验室，团队负责人李庆博士长期从事多模态理解、多模态智能体、具身智能等方向，主页：https://liqing.io\n近年来，人工智能正逐步从虚拟的互联网空间（Cyber Space）迈向真实的物理世界（Physical Space）[1]。这一转变的核心挑战之一，是如何赋予智能体对三维空间的理解能力 [2]，实现自然语言与真实物理环境的对齐（grounding）。尽管已有的 3D 空间理解模型在视觉感知和语言对齐方面取得了显著进展，但它们普遍依赖于静态的世界的观察，缺乏对\n主动探索行为\n的建模。\n针对这一问题，清华大学、北京通研院、北理工与北航的研究团队联合提出了一种\n统一空间理解与主动探索的新型模型\n。该方法使智能体能够在动态探索过程中逐步构建对环境的认知，从而实现更高效的空间感知与自主导航，为智能体在物理世界中的任务执行奠定了基础。\n这个工作已被 ICCV 2025 接收，所有审稿人一致给出满分评价\n。\n论文标题：Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation\n论文链接：https://arxiv.org/abs/2507.04047\n项目主页：https://mtu3d.github.io\n代码链接：https://github.com/MTU3D/MTU3D\n理解与探索：具身导航中的 “双面镜”\n在具身导航（Embodied Navigation）任务中 [3]，智能体需要根据人类的指令 —— 可能是一句话、一张图片、或者一个任务描述 —— 在复杂的物理空间中找到目标位置。它不仅要 “听懂任务”，还要 “会探索”，这正是空间理解和具身智能中的关键挑战。如下图所示，导航过程中其实包含两个交织进行的关键步骤：\n理解（Grounding）\n：智能体需要先理解指令在空间中具体指的是什么，比如 “去餐桌” 意味着它要找到与 “餐桌” 这个概念匹配的空间位置。\n探索（Exploring）\n：在还不完全了解环境时，智能体必须主动移动、观察和尝试，才能发现目标物品或区域。\n这就像人类在空间中去导航一样，如果你想找吃的，可能会优先去厨房或餐厅看看 —— 这个 “方向感” 正是基于人对空间的理解。而真正找到零食，还需要你在厨房中主动探索。这说明，\n理解和探索不是两个独立的过程，而是互相推动、交替进行的\n。\n研究难点：表征、训练目标和数据采集\n实时语义表征\n：如何构建可在线更新的 3D 语义地图，既包含精确的空间位置，又融合丰富的语义信息，并能持续处理来自 RGB-D 流的数据？\n探索-理解协同训练\n：如何将探索策略与语义理解统一在一个训练目标中联合优化，打破传统模块化方法中两者割裂的问题？\n高效数据采集\n：如何降低真实世界导航数据采集的成本，利用虚拟与真实环境结合，构建自动化、可扩展的数据采集流程？\n核心思路：探索和理解协同训练\n如下图所示，作者将具身导航建模为一个\n探索与视觉落地（grounding）协同进行的闭环过程\n。\n在探索阶段，智能体通过连续的 RGB-D 感知不断积累空间记忆，主动寻找潜在的目标位置。当空间记忆中包含了足够的视觉语义信息后，模型便进入视觉落地阶段 —— 根据语言指令对空间中的候选区域进行匹配，并导航至最符合语义的目标位置。\n这种设计将\n强化学习方法中的主动探索能力\n与\n3D 视觉语言模型的语义理解能力\n统一在一个闭环系统中。\n探索推动理解的发生，理解又反过来引导更高效的探索，从而实现端到端的协同训练与任务执行。\n模型设计和数据采集\n作者提出的模型主要包括两个核心模块：\n在线空间记忆构建\n与\n空间推理与决策\n，二者在统一训练框架下协同优化，实现探索与理解的闭环融合。\n第一部分：在线空间记忆构建\n在每一帧时刻，模型接收来自环境的局部 RGB-D 序列作为输入。每一张图像首先被分别送入\n2D Encoder\n[4] 和\n3D Encoder\n[5] 进行多模态特征编码：其中 2D Encoder 使用 FastSAM [6] 和 DINO 提取语义分割与视觉特征，3D Encoder 基于 Sparse Convolution UNet 提取稀疏体素级别的空间表示。\n随后，这些多模态特征通过一个\nQuery Decoder\n被转化为一组结构化的物体表示（Object Queries），涵盖每个物体的空间位置、体积大小、语义特征和置信度信息 [7,8]。\n同时，系统还利用\nFrontier-based Exploration\n[9] 方法识别尚未探索的空间边界区域，生成对应的 Frontier Queries（表示为 3D 空间坐标点）。\n最终，上述物体与边界信息被写入一个随时间持续更新的\n动态空间记忆库（Dynamic Spatial Memory Bank）\n，为后续的推理与决策提供结构化空间知识。\n第二部分：空间推理\n在推理阶段，系统从空间记忆库中读取当前时刻的 Object Queries 与 Frontier Queries，并与任务文本指令进行\nCross-Attention 融合\n，以识别与语言目标最相关的候选区域。\n该模块具备两种响应机制：\n若语义目标（如 “椅子”）在记忆库中已有匹配物体，模型直接选择其位置进行导航；\n若尚未观测到目标，系统则选择最优的 frontier 区域进行下一步探索，以期在未来观察中获取相关语义信息。\n数据收集过程\n在数据构建方面，作者提出了一种虚实结合的策略，融合了来自真实 RGB-D 扫描数据与虚拟仿真环境的导航轨迹，以综合提升模型的视觉理解与探索能力。\n具体而言，作者从\nScanNet\n[10] 和\nHM3D\n[11] 场景中构建数据：其中，真实轨迹主要来源于 ScanNet 场景的问答与指令任务数据，这些数据包含丰富的视觉-语言对齐信息，有助于提升模型在复杂环境中的语义落地能力。另一方面，基于\nHabitat-Sim\n引擎构建的大规模模拟轨迹，则覆盖了更丰富的空间探索过程，显著增强了模型的主动探索与策略学习能力。\n如下表所示，最终构建的数据集涵盖了超过\n90 万条导航轨迹、上千万级别的语言描述与目标指令\n，并广泛覆盖不同的任务类型，包括视觉指引（VG）、探索行为（Exp）、目标定位（Goal）等。\n实验结果\n作者在四个关键任务上对 MTU3D 进行了全面评估，分别是 HM3D-OVON [12]（支持开放词汇的目标导航）、GOAT-Bench [3]（多模态长期导航）、SG3D-Nav（多步骤任务导航）[13] 和 A-EQA（结合问答的主动探索任务）[14]，展现了模型在多种具身智能场景中的适应能力。\n在 GOAT-Bench 基准测试中，MTU3D 在三个评估集上的\n成功率分别达到 52.2%、48.4% 和 47.2%\n，相比现有方法最高提升超过 20%，显著优于其他模型。该任务涵盖图像、文本、类别等多种目标指令，并要求智能体具备长期记忆能力，一次完成十个以上目标导航。实验结果表明，MTU3D 在\n多模态理解与长期任务规划\n方面展现出强大的泛化能力和稳定表现。\n下表展示了模型在\nSG3D-Nav 时序任务导航\n上的评估结果。相比强化学习方法（如 SenseAct-NN）和模块化方法（如 Embodied Video Agent），MTU3D 在所有指标上均取得\n显著提升\n。该任务要求智能体按照多步语言指令依次完成多个子目标，是对任务规划与长期记忆能力的综合考验。\n作者在 A-EQA 任务中发现，使用 MTU3D 生成的探索轨迹提升了问答表现，GPT-4V 的成功率从 41.8% 提升到 44.2%。 该结果表明 MTU3D 能为多模态大模型提供更高质量的感知输入，助力具身问答任务的发展。\n在消融实验中，作者发现所提出的协同训练策略\nVLE\n在三个导航任务上均带来明显性能提升：在 OVON、GOAT 和 SG3D 任务中，成功率分别提升了\n5.5%、13.9% 和 5.0%\n。这一结果表明，VLE 有效促进了视觉理解与空间探索的协同，显著增强了模型在多任务导航场景中的表现。\n模拟器中的结果展示，在多种任务下，模型能够准确理解目标指令，并成功完成图像导航、语言定位和多步骤操作等复杂任务。\n作者还进行了真机实验，以下三段视频展示了模型在真实世界的能力。\n总结\n我们正处在人工智能从 “屏幕里的世界” 走向 “真实物理世界” 的关键阶段。让 AI 不仅能看懂图像、听懂语言，更要能在真实空间中自主移动、理解指令、完成任务。\nMTU3D\n这一工作的出现，将 “理解” 和 “探索” 结合在一起，让 AI 像人一样，一边探索环境，一边理解指令，逐步建立起对周围世界的认知。通过结合真实和虚拟的数据训练，\nMTU3D\n不仅在模拟器中表现出色，也可以在真实机器人上完成任务，给未来的具身导航提供了新的思路和更多的想象空间。\n参考文献：\n[1] Liu, Y., et al. \"Aligning cyber space with physical world: A comprehensive survey on embodied ai. arXiv 2024.\" arXiv preprint arXiv:2407.06886.\n[2] Zhu, Ziyu, et al. \"3d-vista: Pre-trained transformer for 3d vision and text alignment.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n[3] Khanna, Mukul, et al. \"Goat-bench: A benchmark for multi-modal lifelong navigation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n[4] Caron, Mathilde, et al. \"Emerging properties in self-supervised vision transformers.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n[5] Liu, Baoyuan, et al. \"Sparse convolutional neural networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.\n[6] Zhang, Chaoning, et al. \"Faster segment anything: Towards lightweight sam for mobile applications.\" arXiv preprint arXiv:2306.14289 (2023).\n[7] Zhu, Ziyu, et al. \"Unifying 3d vision-language understanding via promptable queries.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024.\n[8] Xu, Xiuwei, et al. \"Embodiedsam: Online segment any 3d thing in real time.\" arXiv preprint arXiv:2408.11811 (2024).\n[9] Yamauchi, Brian. \"Frontier-based exploration using multiple robots.\" Proceedings of the second international conference on Autonomous agents. 1998.\n[10] Dai, Angela, et al. \"Scannet: Richly-annotated 3d reconstructions of indoor scenes.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n[11] Ramakrishnan, Santhosh K., et al. \"Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai.\" arXiv preprint arXiv:2109.08238 (2021).\n[12] Yokoyama, Naoki, et al. \"HM3D-OVON: A dataset and benchmark for open-vocabulary object goal navigation.\" 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2024.\n[13] Zhang, Zhuofan, et al. \"Task-oriented sequential grounding in 3d scenes.\" arXiv preprint arXiv:2408.04034 (2024).\n[14] Majumdar, Arjun, et al. \"Openeqa: Embodied question answering in the era of foundation models.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024."}
{"title": "英伟达&MIT等推出Long-RL，长视频训练速度翻倍", "published_at": "2025-07-14 11:09:00", "url": "https://www.jiqizhixin.com/articles/2025-07-14-2", "content": "请想象……\n一个 AI—— 它要完整看完一场几十分钟的世界杯决赛，不只是数球门数，更要跨越上百个镜头的线索、情绪、战术细节，甚至要像人一样推断：\n谁会赢点球大战？\n足球比赛预测分析\n预测《\n星际争霸 2》这样的即时战略游戏同样\n需要考虑许多不同的变量\n，难度也非常巨大。\n星际争霸 2 比赛预测分析\n再换个场景：同样是 AI，在一场紧张的德州扑克超级豪客赛上，面对职业牌手的每一次下注、加注、弃牌，能否像一个顶尖牌手一样，推理出对手藏在手里的那两张底牌？\n德州扑克比赛猜牌\n不只是「看」，还要记住所有公共牌、下注顺序、翻牌后的心理博弈，甚至对手的打法偏好 —— 然后在最后一张河牌翻开时，做出\n最优推断\n。\n再换一个小游戏：三只杯子，一颗小球。人盯着屏幕都可能跟丢，AI 能不能像魔术师一样，在上百帧交换里牢牢盯住那颗小球的位置？\n移动杯子猜测小球位置\n这背后，AI 需要的不只是「识别」，更是\n跨时域、跨模态的推理、记忆和博弈洞察\n。\n这，正是 Long-RL 想要解决的挑战：\n如何让大模型在面对长视频和复杂策略推理时，不只是看见，更能理解和推演\n。\n今天，视觉语言模型（VLM）和大语言模型（LLM）越来越强，但现实里，当它们需要处理\n小时级视频、多模态输入、需要长时一致性和上下文推理\n时，传统的开源方案往往力不从心。\n要跑长序列？显存炸了。\n要多模态？上下游兼容难。\n要 RL 高效？采样慢，回报低。\n针对这些难题，英伟达近日联合 MIT、香港大学、UC Berkeley 重磅推出\nLong-RL\n，其能提升 RL 训练数据长度上限，让训练速度翻倍。\n论文：Scaling RL to Long Videos\n项目地址：https://github.com/NVlabs/Long-RL\n论文链接：https://arxiv.org/abs/2507.07966\n简单来说，\nLong-RL\n是一个真正面向长序列推理和多模态强化学习的全栈训练框架。\n支持小时级长视频 RL\n：单机可稳定训练 3600 帧（256k tokens）。\nLong-RL 的核心是 MR-SP 并行框架\nMR-SP\n的全称是\nMulti-modal Reinforcement Sequence Parallelism\n，即多模态强化序列并行，可在不同帧数下显著降低长视频推理的训练耗时和显存：启用 MR-SP 后，训练速度提升可达 2.1×，而传统方案会因显存不足直接 OOM。\n那么，这是如何做到的呢？具体来说，MR-SP 分为两个阶段。\nMulti-modal Reinforcement Sequence Parallel (MR-SP) 系统\n其中，第 1 阶段是\n使用并行编码的 Rollout\n。\n为了高效地支持长视频强化学习，该团队在视频编码阶段采用了\n序列并行 (SP)\n机制。\n如上图左所示，输入视频帧首先会被均匀地分配到多台 GPU（例如，GPU 1 至 GPU 3）上，每台 GPU 都配备了各自的视觉塔（vision tower）。每台 GPU 独立处理视频的一部分，并且仅对其中一部分帧进行编码。然后，生成的视频嵌入将通过 all-gather 操作与文本嵌入进行聚合，如图中 All-Gather 箭头所示。此策略可分散编码工作负载，使系统能够利用更多 GPU 来处理更长的视频，同时避免 GPU 内存溢出的风险。\n并行编码方案可确保视觉塔的均衡利用，并实现可扩展的长视频处理，而这在单台设备上是无法实现的。\n视频嵌入在被全局收集后，将在整个强化学习流程中被下游重复使用。\n如上图所示，收集到的嵌入在多次 rollout 过程中可重复使用，且无需重新计算。例如，在每个训练步骤中，通常会执行 8 到 16 次 rollout。如果不进行回收，同一视频每一步都需要重新编码数十次，这会严重影响训练速度。通过缓存和重用收集到的嵌入，MR-SP 可消除这种冗余，并显著加快训练速度。\n第 2 阶段则是\n使用序列并行进行预填充\n。\n对于每次 rollout，参考模型和策略模型都需要在强化学习中对长视频进行计算密集型预填充。通过复用第 1 阶段收集到的嵌入，可使用序列并行在各个设备之间并行化推理阶段。\n如上图右所示，这里的方案是全局收集输入嵌入 —— 这些嵌入首先会被填充到统一长度（Padding Sequence），然后均匀地分配到各台 GPU（Sharding to Local GPU）。\n这样一来，每台 GPU 在预填充期间只需处理输入序列的一部分。这种并行性适用于策略和参考模型的预填充。然后，每台 GPU 会在本地计算其 token 切片的 logit，并且并行进行预填充。\nLong-RL 也是一个多模态 RL 工具箱\n该团队也将 Long-RL 打造成了一个完整的多模态 RL 工具箱，能适配：\n多模型\n：除了 VILA 系列、Qwen/Qwen-VL 系列这些 LLMs/VLMs，也支持 Stable Diffusion、Wan 等生成模型。\n多算法\n：GRPO、DAPO、Reinforce，一行切换。\n多模态\n：不仅文本，视频、音频一起上。\nLongVILA-R1\n使用 Long-RL，英伟达的这个团队构建了\nLongVILA-R1\n训练框架。从名字也能看到出来，这个训练框架基于\nVILA\n—— 一个同样来自该公司的视觉-语言模型（VLM），详见论文《VILA: On Pre-training for Visual Language Models》。\n训练流程方面，LongVILA-R1 基于 LongVILA 的基础训练流程，然后进一步使用 MM-SP 以通过长 CoT 在长视频理解任务进行 SFT。然后，通过多模态强化序列并行 (MR-SP) 进行强化 scaling 学习。\nLongVILA-R1 训练流程\n框架上，LongVILA-R1 集成了 MR-SP 来实现可扩展视频帧编码和 LLM 预填充。强化学习采用了基于 vLLM 的引擎，并带有缓存的视频嵌入，并针对 LongVILA rollout 进行了定制。针对准确度和格式的奖励将作为策略优化的引导。\nLongVILA-R1 强化学习训练框架\nLongVILA-R1 可以说是 Long-RL 的「明星学员」，专门攻克长视频推理这块硬骨头。\n总结起来，它的创新点可以用三个关键词概括：\n大规模高质量数据 LongVideo-Reason\n：52K 长视频推理样本，涵盖 Temporal / Goal / Spatial / Plot 四大类推理。\n两阶段训练\n：先用 CoT-SFT 把链式推理打基础，再用 RL 强化泛化，学得更稳更深。\nMR-SP 高效并行\n：多模态长序列并行，特征可复用，一次缓存多次用。\n大规模数据集 LongVideo-Reason\n效果如何？\n在 LongVideo-Reason-eval 这种强推理基准上，随着帧数增加，加入推理显著提高了准确度，并且相比无推理设置优势逐渐扩大。\n该团队也通过消融实验验证了各组件的有效性。\n在真实世界里，无论是看一场完整的足球赛、跟人多轮对话，还是让机器人长时间工作，都需要 AI 能在长时间里保留上下文、持续推理，并根据反馈自我调整。这正是强化学习（RL）擅长的：不断试错、获取回报、做出更优决策。\n该团队表示：\n只有把 RL 和长序列推理结合起来，AI 才可能跨越「一次推理」走向「持续智能」\n—— 这也是 AGI 的必经之路。\n研究团队\n陈玉康\n现任 NVIDIA 研究科学家，于香港中文大学获得博士学位，从事大语言模型（LLM）、视觉语言模型（VLM）、高效深度学习等方面研究。目前已在国际顶级会议和期刊发表论文 30 余篇；多项研究成果在 ICLR、CVPR 等顶级会议上获选口头报告，并在 Google Scholar 上累计引用超过 5,000 次，代表作包括 VoxelNeXt, LongLoRA, LongVILA, Long-RL. 他作为第一作者主导的多个开源项目在 GitHub 上已获得超过 6,000 星标。并在包括 Microsoft COCO、ScanNet 和 nuScenes 等多个国际知名竞赛和榜单中取得冠军或第一名的成绩。\n黄炜\n，香港大学二年级博士生。主要研究方向为轻量化（多模态）大语言模型，神经网络压缩以及高效多模态推理模型训练，在 ICML、ICLR、CVPR 等会议和期刊发表多篇文章。在 NVIDIA 实习期间完成此工作。\n陆垚\n现任 NVIDIA 杰出科学家，UCSD 博士。目前主要研究方向为视觉语言模型和视觉语言动作模型。他是开源视觉语言模型 VILA 系列的负责人。在加入 NVIDIA 之前，他是 Google DeepMind 的研究经理，曾一起领导研发 SayCan, RT-1, RT-2 等具身智能领域的奠基性工作。\n韩松\n是 MIT 电气工程与计算机科学系副教授、NVIDIA 杰出科学家，斯坦福大学博士。他提出了广泛用于高效 AI 计算的「深度压缩」技术，并首创将权重稀疏性引入 AI 芯片的「高效推理引擎」，该成果为 ISCA 50 年历史引用量前五。他的团队致力于将 AI 模型优化、压缩并部署到资源受限设备，提升了大语言模型（LLM）和生成式 AI 在训练和推理阶段的效率，成果已被 NVIDIA TensorRT-LLM 采用。他曾获 ICLR、FPGA、MLSys 最佳论文奖，入选 MIT 科技评论「35 岁以下科技创新 35 人」，并获得 NSF CAREER 奖、IEEE「AI’s 10 to Watch」奖和 Sloan 研究奖。"}
{"title": "OpenAI的o3在新的「解答科学问题AI排行榜」上排名第一，DeepSeek的R1排名第二", "published_at": "2025-07-14 10:17:38", "url": "https://www.jiqizhixin.com/articles/2025-07-14", "content": "编辑 | 白菜叶\n根据近期推出的基准测试平台，o3 是由 ChatGPT 的创建者开发的人工智能 (AI) 模型，被评为回答多个领域科学问题的最佳 AI 工具。\n由华盛顿州西雅图艾伦人工智能研究所 (Ai2) 开发的 SciArena，根据 23 个大型语言模型 (LLM) 对科学问题的回答进行了排名。102 位研究人员对答案的质量进行了投票。\n由 OpenAI 开发的 o3，在回答自然科学、医疗保健、工程以及人文和社会科学问题方面被评为最佳。\nSciArena：https://allenai.org/blog/sciarena\n由 DeepSeek 公司研发的 DeepSeek-R1 在自然科学问题上排名第二，在工程学问题上排名第四。谷歌的 Gemini-2.5-Pro 在自然科学问题上排名第三，在工程学和医疗保健问题上排名第五。\n图示：部分排名展示。（来源：SciArena 官网）\nAi2 的研究科学家 Arman Cohan 表示，用户对 o3 的偏好可能源于该模型倾向于提供大量引用文献的细节，并给出技术上细致入微的响应。但解释模型性能差异的原因颇具挑战性，因为大多数模型都是专有的。他表示，训练数据和模型优化目标等差异可能部分解释了这一点。\nSciArena 是最新开发的平台，旨在评估 AI 模型在某些任务上的表现，也是首批利用众包反馈对科学任务的表现进行排名的平台之一。澳大利亚国立大学机器人与人工智能研究员 Rahul Shome 表示：「SciArena 是一项积极的尝试，它促使人们认真评估 LLM 辅助的文献任务。」\n随机选择\n为了对这 23 个 LLM 项目进行排名，SciArena 邀请研究人员提交一些科学问题。研究人员从两个随机选择的模型中获得了答案，这些模型引用了 Semantic Scholar（一款同样由 Ai2 开发的人工智能研究工具）的参考文献，以支持他们的回答。之后，用户投票选出其中一个模型提供了最佳答案，两个模型之间没有太大区别，或者两个模型都表现不佳。\n该平台现已向公众开放，用户可免费提出研究问题。所有用户均可获得两个模型的答案，并可对其表现进行投票，但只有经过验证并同意相关条款的用户的投票才会被纳入排行榜。该公司表示，排行榜将定期更新。\n澳大利亚悉尼大学人工智能研究员 Jonathan Kummerfeld 表示，能够就科学话题向 LLM 提问，并对答案充满信心，将有助于研究人员掌握其领域的最新文献。「这将帮助研究人员找到他们可能错过的研究成果。」\nKummerfeld 表示，该平台还可以推动人工智能模型的创新，因为排行榜提供了一种透明的进度衡量方式。他补充说，该平台似乎经过精心设计，可以避免用户操纵分数等问题——其他基准测试平台也存在类似的问题。\nKummerfeld 表示，一个潜在问题是该平台对用户参与的依赖。「这些用户付出时间换取使用该工具的权利。」他说道。\n「只要他们认为交易划算，它就能成功；但如果他们觉得自己没有获得价值，平台可能难以获得足够的参与。」 Cohan 表示，该平台通过免费提供并包含最新模型来激励用户。此外，Semantic Scholar 提供的参考文献表明，这些回复对研究人员「有用」。\nShome 表示，科学家应该牢记，LLM 撰写的文本可能与被引用的论文存在冲突，可能误解术语，并且可能无法准确回答问题。他补充道，阅读 LLM 撰写的研究论文摘要并不能代替阅读论文。\n关于 SciArena\nSciArena 是一个开放式评估平台，研究人员可以在此比较和投票评估不同基础模型在科学文献相关任务中的表现。它采用社区投票的方式构建，类似于 Chatbot Arena，但专门针对科学探究的复杂性和开放性进行了定制。\n图示：截至 2025 年 6 月 30 日，SciArena 平台收集的人类偏好数据在各个科学学科的分布情况。（来源：SciArena 官网）\n该平台由三个主要部分组成：\nSciArena 平台：\n人类研究人员在此提交问题，并排查看来自不同基础模型的答案，并为首选结果投票。\n排行榜：\n基于社区投票，Elo 评分系统对模型进行排名，提供动态且最新的性能评估。\nSciArena-Eval：\n这是一个基于收集的人类偏好数据的元评估基准，旨在评估基于模型的评估系统的准确性。\n相关报道：https://www.nature.com/articles/d41586-025-02177-7"}
{"title": "AI编程「反直觉」调研引300万围观！开发者坚信提速20%，实测反慢19%", "published_at": "2025-07-13 21:59:59", "url": "https://www.jiqizhixin.com/articles/2025-07-13-3", "content": "随着大模型的崛起，AI编程领域正在发生翻天覆地的变化。\n各种编程大模型、编程工具涌现，通过自动补全代码、自动 debug 等实用的功能为开发者的日常工作提供极大便利，并在一定程度上提升了开发效率。\n不过，问题来了，AI 编程工具带来的影响真是如此吗？\n近日，一家非营利性 AI 调研机构「METR」进行了一项随机对照实验，旨在了解 AI 编程工具如何加速经验丰富的开源开发者的工作效率。\n结果却是非常令人意外：\n开发者本来坚信使用使用 AI 工具后速度可以提升 20%，但实际上速度却比没有使用 AI 工具时慢了 19%。\n这一结论在社交媒体 X 上爆了，阅读量几乎要突破 300 万。\n如下图所示：与专家预测和开发者本来的直觉相反，2025 年初的 AI 编程工具将减缓经验丰富的开发者的开发速度。在本次随机对照实验中，16 位拥有中等 AI 编程经验的开发者完成了 246 项大型复杂项目的任务，他们平均拥有 5 年开发经验。\n对于这项调研，网友反应不一。有人感同身受，表示赞同。\n有人则对 METR 的测量指标产生了质疑，表示「完成任务的时间」不能与「生产力」划等号，并且中等 AI 经验与复杂项目测试同样是糟糕的设置。\n「应该调研普通人（非开发者）使用 AI 编写软件的速度快了多少」，也有人提出了这样的建议。\n研究动机\n虽然编码 / 智能体基准测试已被证明有助于理解 AI 能力，但它们通常为了规模和效率而牺牲了真实性 —— 这些任务自成一体，不需要理解先前的上下文，并且使用算法评估，无法捕捉许多重要的能力。这些特性可能导致基准测试高估了 AI 的能力。\n另一方面，由于基准测试是在没有真人实时交互的情况下运行的，模型可能尽管取得了实质性进展却未能完成任务，比如一些在真实场景中人类会轻松修复的小瓶颈。总的来说，直接将基准测试分数转化为现实世界的影响是困难的。\nMETR 表示，评估 AI 对软件开发人员生产力的影响，可以为基准测试提供补充证据，有助于了解 AI 对 AI 研发加速的整体影响。\n方法、结果与原因分析\n为了直接测量 AI 工具在软件开发中的现实影响力，\nMETR 招募了 16 位经验丰富的开发者\n，他们来自大型开源仓库（平均拥有 22k+star 和 100 万 + 行代码），并已贡献多年。\n这些开发者提供了他们认为对仓库有价值的真实问题列表（总计 246 个）—— 即通常属于他们日常工作范畴的 bug 修复、功能开发和重构任务。\n接下来，METR 随机分配每个问题，允许或禁止开发者在处理该问题时使用 AI。当允许使用 AI 时，开发者可以选择他们喜欢的任何工具（研究期间主要使用 Cursor Pro 搭配 Claude 3.5/3.7 Sonnet）。\n当禁止使用时，他们则在没有生成式 AI 辅助的情况下工作。开发者在完成任务（平均每个耗时约两小时）时录屏，并自行报告完成所需的全部时间。\n当然，METR 按每小时 150 美元（约合人民币 1080 元 / 小时）的标准向开发者支付参与研究的报酬。\n结果显示：当开发者被允许使用 AI 工具时，他们解决问题所需的时间反而增加了 19%—— 这一显著的效率下降与专家预测以及开发者的感知背道而驰。\n这种感知与现实之间的差距令人震惊：开发者预期 AI 能将其速度提升 24%，即使在经历了效率下降之后，他们仍然相信 AI 可以为他们提速 20%。\n下文展示了开发者预测的平均时间和实际观察到的实现时间。我们可以清晰地看到，当开发者被允许使用 AI 工具时，他们花费的时间明显更长。\n当允许使用 AI 时，\n开发者在主动编码和查找信息上的时间减少了，取而代之的是花更多时间撰写提示词、等待或审查 AI 输出，以及处于空闲状态。\nMETR 发现，开发进度的放缓并不是单一原因，而是由多种因素共同导致的。\n为了更好地理解这些因素，\nMETR 考察了实验环境中的 20 项属性\n，发现其中有 5 项很可能是造成开发速度放缓的原因，另外有 8 项则表现出混合或不明确的影响。\n过程中，METR 排除了许多实验干扰因素，比如开发者使用了前沿模型、遵守了处理分配规则、没有选择性地放弃问题（例如放弃困难的不使用 AI 任务从而降低不使用 AI 组的平均难度），并且在使用和不使用 AI 的情况下都提交了质量相近的 PR（Pull Request）。\n结果发现：无论是采用不同的结果指标、估计方法，还是对数据进行各种子集 / 子分析，开发速度的放缓现象依然存在。\n更详细的调研结果请参考原论文：\n论文标题：Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity\n论文地址：https://metr.org/Early_2025_AI_Experienced_OS_Devs_Study.pdf\n局限性与未来展望\n此次调研得出了两个重要结论，分别是：\n在某些重要场景下，近期的 AI 工具有可能并未提升生产力，甚至可能导致效率下降。\n关于效率提升的自我报告并不可靠 —— 要真正理解 AI 对生产力的影响，我们需要真实环境中的实验数据。\n不过，METR 也表示，他们的设置并没有代表所有（甚至可能是大多数）软件工程，同时声明当前的模型也能更有效地利用起来，未来的模型可能会变得更好。\n当然，没有哪种测量方法是完美的 —— 人们希望 AI 系统完成的任务是多样、复杂且难以严格研究的。各种方法之间存在有意义的权衡，继续开发和使用多样化的评估方法以更全面地描绘 AI 的现状和未来发展方向，将至关重要。\n未来，METR 期待运行类似的 AI 调研，以追踪 AI 带来的加速（或减速）趋势，这类评估方法可能比基准测试更难被「玩弄」。\n博客地址：https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/"}
{"title": "「流匹配」成ICML 2025超热门主题！网友：都说了学物理的不准转计算机", "published_at": "2025-07-13 21:56:20", "url": "https://www.jiqizhixin.com/articles/2025-07-13-2", "content": "流体力学融入生成式 AI ，构建了一种非常简洁、优雅的形态。\n众所周知，第 42 届国际机器学习大会（ICML）将于 7 月 13 日至 19 日在加拿大温哥华盛大举行。\n在生成式 AI 领域，最新的前沿热点已经转向探索更高质量，更稳定，更简洁，更通用的模型形态。\n流匹配（Flow Matching）技术\n正完美的踩中了每一个热点要素。\n自从 FLUX 模型发布后，能够处理多种输入类型的流匹配架构逐渐成为目光焦点。\n也因此有学者感慨，在 ICML 2025 的生成相关工作中，流匹配技术几乎\n无处不在\n。\n流匹配技术虽说在生成式 AI 领域是前沿研究，但其核心概念来源于流体力学。\n令人惊讶的是，物理领域的有关概念在近些年的确为生成领域的研究提供了很多新方向和新成果。\n甚至\n薛定谔桥都能用在扩散生成领域\n！\n在知乎相关技术解读专栏《深入解析 Flow Matching 技术》下，网友怒评：物理学专业的不准转计算机！\n专栏标题：《深入解析 Flow Matching 技术》\n专栏链接：https://zhuanlan.zhihu.com/p/685921518\n本文参考研究者 Floor Eijkelboom 的最新推文，从原理入手，避免繁杂的数学公式，来介绍这一简洁优雅且高效的生成技术。\n生成：噪声映射到数据\n生成工作是一个逐步具象化的过程，从一个抽象的表示开始，通过不同的生成网络，最终生成出具有复杂细节的真实数据。在此过程中，我们希望从一个无序的「噪声分布」映射到不同的复杂的数据分布中，这种映射是高度非线性的，而且存在无限的可能性。\n生成猫猫 由噪声向图像映射\n从本质上讲，流匹配的核心思想非常简单：\n学习\n将噪声转化为数据\n。\n我们首先在噪声分布与数据分布之间选择一种\n插值方式\n（如图所示）。\n流匹配会学习如何沿着这条插值路径移动每一个样本，将起始时刻（time 0）的噪声点逐步转化为终点时刻（time 1）对应的数据点。\n流匹配是基于\n归一化流\n（Normalizing Flows，NF）的生成模型。它通过一系列可逆的变量变换，将复杂的概率分布逐步映射为简单的分布；同时，也可以通过这些变换的逆过程，从简单分布中生成逼真的数据样本。\n流匹配原理：流体力学\n连续性方程\n那么，噪声点向数据点的差值路径应当如何建立？\n这个问题已经在\n流体动力学\n等领域中得到研究！\n在流体中，追踪每一个微小粒子的运动轨迹是明显困难的。因此我们更关注的是：每个空间区域内\n平均\n存在多少水？这种平均量被称为\n密度\n。\n为了研究密度的变化，物理学提供了一个重要工具：\n连续性方程（continuity equation）\n。\n连续性方程建立在一个简单而基本的原理之上：质量既不会凭空产生，也不会无故消失。\n这一原理不仅适用于物理质量，同样适用于概率质量（probability mass）。这直接建立了\n物理概念与生成模型中概率分布\n的直接联系。\n连续性方程：同样适用于概率质量\n直观的理解连续性方程：\n如果流入的密度大于流出，则该点密度增加；\n如果流出大于流入，则密度减少；\n如果两者相等，密度保持不变（即处于平衡状态）。\n这种 「总流出量」 被称为\n散度（divergence）\n。\n在物理学中，我们通常是从粒子的运动行为出发，推导出整体密度的变化规律。\n但\n流匹配正好相反\n！它从一开始就指定密度的变化过程 —— 即从噪声分布逐步过渡到数据分布的插值轨迹 —— 然后去学习使这一演化成立的\n速度场（velocity field）\n。正是这个速度场，使得我们能够从噪声中生成新的数据样本。\n过程示意\n我们先从一个简单的情况开始 —— 只考虑一个数据点。\n在这种情况下，我们通过从噪声点到该数据点之间的直线路径来定义变化过程。也就是在路径上的每一个位置，其速度方向都直接指向目标数据点。\n由于这个过程是针对特定数据点定义的，我们称之为\n条件流（conditional flow）\n。\n流匹配的「魔法」，在于它如何处理整个数据分布。\n在空间中的任意一点，都可能会有无数条从噪声出发、通向不同数据点的插值路径穿过。而此时，我们需要的总体速度场，就是这些\n路径在该点的平均方向\n。\n训练过程（学习平均插值速度场）与生成过程的示意\n具体原因如下：\n在空间中的任意一点，可能存在多条从噪声出发、通向不同数据点的路径经过它，这些路径可能通向高概率的样本，也可能通向低概率的样本。\n但对于这个特定位置来说，\n更可能处在属于通向高概率样本的路径上\n。因此，在这个位置上，穿过它的所有路径的平均方向正好反映了这一点，如图所示。\n流匹配有一个\n对偶视角\n，称为\n变分流匹配（Variational Flow Matching, VFM）\n。\n与其在每个位置上对所有路径的速度进行平均，VFM 的思路是：在空间中的每一点，推断它可能朝向的\n终点分布\n。这样一来，该点的速度场就简单地指向这个分布的均值。\n当变分后验为高斯分布时，流匹配与变分流匹配是等价的。\n我们从数据分布中采样一个数据点 x_1，从噪声分布中采样一个噪声点 x_0，并在它们之间插值得到某个中间点 x_t。接下来流匹配学习的是：在该位置\n应该朝哪个方向移动\n；\n下方展示了对应的伪代码：\n若对流匹配感兴趣，可以参阅以下论文：\n论文标题：Flow Matching for Generative Modeling\n论文链接：https://arxiv.org/abs/2210.02747\n扩散与流匹配的对比\n一体两面\n读到这里，不难发现流匹配的方法和扩散模型的逻辑非常相似，似乎具备完全相同的前向过程。\n那么扩散模型和流匹配具有怎样的关系呢？\nMIT 副教授何恺明认为\n，流匹配技术在生成模型领域的位置，扩散模型是流匹配的子集：\n实际上，当采用\n高斯分布作为插值策略时，扩散模型其实就是一种特殊的流匹配\n。\n这是一个好消息，这意味着你可以互换使用这两个框架。\n关于扩散模型与流匹配的训练过程：\n权重函数的一致性：\n训练中使用的权重函数十分关键，它决定了模型如何平衡感知数据中不同频率成分的重要性。流匹配中的权重设计恰好与扩散模型文献中常用的训练权重函数高度一致。\n噪声调度对训练目标影响较小：\n虽然噪声调度对训练效率存在影响，但它对最终的训练目标函数本身作用不大。\n网络输出形式的差异：\n流匹配提出了一种新的网络输出形式\n扩散模型与高斯流匹配（Gaussian Flow Matching）在\n本质上是等价\n的。\n但值得注意的是，高斯流匹配为生成建模领域带来了两个新的模型设定：\n网络输出形式：\n流匹配提出了一种新的网络输出参数化方式，将其视为一个速度场，这与传统扩散模型中的输出形式不同。在使用高阶采样器时，这种输出形式可能带来性能差异，并可能影响训练过程中的动态行为。\n采样噪声调度：\n流匹配使用了一种简单的采样噪声调度策略，其更新规则与 DDIM 相同。\n对扩散模型和流匹配关联的更多信息，请参阅以下文章：\n文章标题：Diffusion Meets Flow Matching: Two Sides of the Same Coin\n文章链接：https://diffusionflow.github.io\n参考内容：\nhttps://x.com/FEijkelboom/status/1942944767563661459\nhttps://mlg.eng.cam.ac.uk/blog/2024/01/20/flow-matching.html\nhttps://zhuanlan.zhihu.com/p/685921518"}
{"title": "VLA 推理新范式！一致性模型 CEED-VLA 实现四倍加速！", "published_at": "2025-07-13 21:41:43", "url": "https://www.jiqizhixin.com/articles/2025-07-13", "content": "本文第一作者为香港科技大学（广州）机器人系一年级博士生宋文轩，主要研究方向为VLA模型，共同第一作者是来自香港科技大学广州的研究助理陈家毅，项目leader为浙江大学和西湖大学联合培养博士生丁鹏翔，他们也是具身智能领域开源项目OpenHelix以及LLaVA-VLA的研究团队。通讯作者为香港科技大学广州的李昊昂教授，他是今年的CVPR2025 Best Paper Candidate的获得者。\n近年来，视觉 - 语言 - 动作（Vision-Language-Action, VLA）模型因其出色的多模态理解与泛化能力，已成为机器人领域的重要研究方向。尽管相关技术取得了显著进展，但在实际部署中，尤其是在高频率和精细操作等任务中，VLA 模型仍受到推理速度瓶颈的严重制约。\n针对这一问题，部分研究提出采用 Jacobi 解码替代传统的自回归解码，以期提升推理效率。然而，由于 Jacobi 解码往往需要较多迭代次数，其加速效果在实践中较为有限。\n为此，我们提出了\n一种一致性蒸馏训练（consistency distillation training）策略\n，使模型在每次迭代中能够同时预测多个正确的动作 token，从而实现解码加速。同时，我们设计了混合标签监督机制（mixed-label supervision），用于缓解蒸馏过程中可能产生的误差积累问题。\n尽管上述方法带来了可接受的加速效果，我们进一步观察到：Jacobi 解码中仍存在若干低效迭代步骤，成为限制整体效率的关键瓶颈。为彻底解决该问题，本文提出一种提前退出（early-exit）解码策略，通过适度放宽收敛条件，进一步提升平均推理效率。\n论文题目：CEED-VLA : Consistency Vision-Language-Action Model with Early-Exit Decoding\n项目主页：https://irpn-eai.github.io/CEED-VLA/\n论文链接: https://arxiv.org/pdf/2506.13725\n代码链接: https://github.com/OpenHelix-Team/CEED-VLA\n实验结果表明，我们所提出的方法在多个基线模型上实现了超过4 倍的推理加速，同时在仿真与真实机器人任务中均保持了较高的任务成功率。这些实验验证了本方法在加速机器人多模态决策过程中的高效性与通用性，展现出良好的应用前景。总的来说，我们做出以下三大贡献：\n（1）我们提出了一种通用的加速方法\nCEED-VLA\n，在保持操控性能的前提下显著提升了推理速度。\n（2）我们引入了一种\n一致性蒸馏机制\n，并在自回归损失中结合\n混合标签监督\n，以有效保留高质量的动作序列。\n（3）我们发现 Jacobi 解码存在\n低效迭代\n的瓶颈问题，进一步提出了\n早期退出（early-exit）解码策略\n，实现了\n4.1 倍的推理加速与 超过 4.3 倍的解码频率提升\n。\n图 1：不同解码方法加速效果对比\nMethod\n图 2：CEED-VLA 模型架构示意图\n我们提出的框架首先通过预训练的 VLA 模型（例如 LLaVA-VLA和OpenVLA）进行Jacobi Decoding生成训练Jacobi Trajectory数据集。随后，我们设计了一种高效的一致性蒸馏方式，并引入了一种新颖的混合标签监督方法，在同时保证精度和提高速度的前提下训练学生模型。最后，我们提出了Early-exit Decoding技术，以进一步提升推理速度。模拟环境与现实世界中的实验表明，在几乎不损失任务成功率的前提下，该方法显著提升了模型的推理速度和灵巧任务的成功率。\nConsistency Training\n对于目标 VLA 模型 ，为了捕捉 Jacobi 轨迹中的内在一致性以进行一致性训练，我们首先通过在机器人数据集C上使用 Jacobi Decoding对模型 进行动作预测，来采集完整的Jacobi轨迹。\n一致性训练包含两个优化目标：\n一致性损失(Consistency Loss)\n: 引导模型能够\n在单次forward过程中预测多个正确的 token\n，为了确保模型在轨迹中的任意一步都能生成与最终目标一致的动作，这里引入了 KL 散度作为一致性损失。简而言之，它要求模型在每一个中间步骤的预测，和最终预测结果之间保持一致，从而提高模型收敛效率。\n混合标签的自回归监督损失(Mixed-label AR Supervision)\n: 为了保留模型常规的自回归生成能力，CEED-VLA混合使用教师模型的数据以及Ground-truth数据进行监督，以保证动作精确性。最终的训练目标是两种损失的加权和。训练过程如下所示:\n图4  一致性训练算法\nEarly-exit Decoding\n图 5：四种解码方式迭代流程\nJacobi 解码允许并行输出动作token，在一定程度上提高了推理速度，但严格的收敛条件影响解码效率进一步提升。为此我们提出Early-exit Decoding策略：模型通过提前退出的方式输出中间预测结果，无需满足Jacobi iteration的收敛条件。得益于manipulation任务独特的结构，Early-exit Decoding显著提升了推理速度，同时保持了成功率，使得模型能够以更高频率控制机器人，满足实时任务需求。\n仿真环境基准实验(Simulation Benchmark)\n图 6：仿真环境主要实验结果\n在最具挑战的长程任务CALVIN ABC-D和LIBERO-Long基准上的实验结果表明，CEED-VLA在几乎不损失任务成功率的前提下实现了4倍以上的推理速度和执行频率。\n真实世界实验（Real World）\n图 9：真机实验部署设置\n图 10：叠毛巾任务上的对比\n上图展示了 LLaVA-VLA 模型的真实表现。机械臂操作频率较低，难以完成如叠毛巾等灵巧操作任务，经常出现抓取失败或只抓到一边的情况，导致任务失败。下图展示了 CEED-VLA 模型的实验效果。得益于推理频率的提高，机械臂动作更加顺畅，成功完成了灵巧操作任务。\n图 11：CEED-VLA 在真实世界中的实验结果。\nCEED-VLA 显著提升了推理速度和控制频率，使模型能够学习并执行高频动作，因此相比基线在灵巧任务上的成功率大幅提升，超过 70%。"}
