<!DOCTYPE html>
<html lang="en" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="摘要：本文介绍了一种利用大语言模型内部奖励机制解决对齐问题的方法，称为内源性奖励模型。研究表明，这种内源性奖励模型不仅可以超越传统基于人类标注数据的奖励模型，还可以提高模型的性能，改进策略学习过程。\n主题标签：人工智能、大数据、机器学习\n原文摘要 将大语言模型（LLMs）与复杂的人类价值观对齐，仍然是 AI 面临的一个核心挑战。当前主要的方法是基于人类反馈的强化学习（RLHF）。该流程依赖于一个通过人类偏好训练的奖励模型来对模型输出进行评分，最终对齐后的 LLM 的质量在根本上取决于该奖励模型的质量。 因此，创建一个先进的奖励模型需要建立庞大且高质量的人类偏好数据集，而这一过程通常既缓慢、昂贵，又难以扩展。\u00a0这种对人类标注数据的依赖促使研究者探索其他对齐方法。一个重要的研究方向是基于 AI 反馈的强化学习（RLAIF）。该方法利用强大的专有大语言模型生成奖励信号或偏好标签，从而规避人类标注需求。虽然成本效益显著，但这些方法缺乏严谨的理论基础，且容易继承评判模型本身的风格偏差与固有偏见。这引发了一个关键问题： 高质量奖励信号是否必须依赖外部来源？ 来自南京大学的研究者发现， 一个强大的通用奖励模型并非需要构建，而是可以挖掘出来的 ， 因为它已经潜在地存在于通过标准的下一个 Token 预测训练的任何语言模型中，称之为「内源性奖励（endogenous reward）」。 本文的核心贡献是为这一观点提供严格的理论基础。本文证明了可&hellip;\n">
<title>周志华团队新作：LLM中存在奖励模型，首次理论证明RL对LLM有效性</title>

<link rel='canonical' href='https://caxlee.github.io/p/%E5%91%A8%E5%BF%97%E5%8D%8E%E5%9B%A2%E9%98%9F%E6%96%B0%E4%BD%9Cllm%E4%B8%AD%E5%AD%98%E5%9C%A8%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%E9%A6%96%E6%AC%A1%E7%90%86%E8%AE%BA%E8%AF%81%E6%98%8Erl%E5%AF%B9llm%E6%9C%89%E6%95%88%E6%80%A7/'>

<link rel="stylesheet" href="/scss/style.min.ee36f3b1c262e9ee31e77636bd21600aa5fbc08e33d66a40000d925ec90e18a3.css"><meta property='og:title' content="周志华团队新作：LLM中存在奖励模型，首次理论证明RL对LLM有效性">
<meta property='og:description' content="摘要：本文介绍了一种利用大语言模型内部奖励机制解决对齐问题的方法，称为内源性奖励模型。研究表明，这种内源性奖励模型不仅可以超越传统基于人类标注数据的奖励模型，还可以提高模型的性能，改进策略学习过程。\n主题标签：人工智能、大数据、机器学习\n原文摘要 将大语言模型（LLMs）与复杂的人类价值观对齐，仍然是 AI 面临的一个核心挑战。当前主要的方法是基于人类反馈的强化学习（RLHF）。该流程依赖于一个通过人类偏好训练的奖励模型来对模型输出进行评分，最终对齐后的 LLM 的质量在根本上取决于该奖励模型的质量。 因此，创建一个先进的奖励模型需要建立庞大且高质量的人类偏好数据集，而这一过程通常既缓慢、昂贵，又难以扩展。\u00a0这种对人类标注数据的依赖促使研究者探索其他对齐方法。一个重要的研究方向是基于 AI 反馈的强化学习（RLAIF）。该方法利用强大的专有大语言模型生成奖励信号或偏好标签，从而规避人类标注需求。虽然成本效益显著，但这些方法缺乏严谨的理论基础，且容易继承评判模型本身的风格偏差与固有偏见。这引发了一个关键问题： 高质量奖励信号是否必须依赖外部来源？ 来自南京大学的研究者发现， 一个强大的通用奖励模型并非需要构建，而是可以挖掘出来的 ， 因为它已经潜在地存在于通过标准的下一个 Token 预测训练的任何语言模型中，称之为「内源性奖励（endogenous reward）」。 本文的核心贡献是为这一观点提供严格的理论基础。本文证明了可&hellip;\n">
<meta property='og:url' content='https://caxlee.github.io/p/%E5%91%A8%E5%BF%97%E5%8D%8E%E5%9B%A2%E9%98%9F%E6%96%B0%E4%BD%9Cllm%E4%B8%AD%E5%AD%98%E5%9C%A8%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%E9%A6%96%E6%AC%A1%E7%90%86%E8%AE%BA%E8%AF%81%E6%98%8Erl%E5%AF%B9llm%E6%9C%89%E6%95%88%E6%80%A7/'>
<meta property='og:site_name' content='Example Site'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:published_time' content='2025-07-02T16:42:32&#43;08:00'/><meta property='article:modified_time' content='2025-07-02T16:42:32&#43;08:00'/>
<meta name="twitter:title" content="周志华团队新作：LLM中存在奖励模型，首次理论证明RL对LLM有效性">
<meta name="twitter:description" content="摘要：本文介绍了一种利用大语言模型内部奖励机制解决对齐问题的方法，称为内源性奖励模型。研究表明，这种内源性奖励模型不仅可以超越传统基于人类标注数据的奖励模型，还可以提高模型的性能，改进策略学习过程。\n主题标签：人工智能、大数据、机器学习\n原文摘要 将大语言模型（LLMs）与复杂的人类价值观对齐，仍然是 AI 面临的一个核心挑战。当前主要的方法是基于人类反馈的强化学习（RLHF）。该流程依赖于一个通过人类偏好训练的奖励模型来对模型输出进行评分，最终对齐后的 LLM 的质量在根本上取决于该奖励模型的质量。 因此，创建一个先进的奖励模型需要建立庞大且高质量的人类偏好数据集，而这一过程通常既缓慢、昂贵，又难以扩展。\u00a0这种对人类标注数据的依赖促使研究者探索其他对齐方法。一个重要的研究方向是基于 AI 反馈的强化学习（RLAIF）。该方法利用强大的专有大语言模型生成奖励信号或偏好标签，从而规避人类标注需求。虽然成本效益显著，但这些方法缺乏严谨的理论基础，且容易继承评判模型本身的风格偏差与固有偏见。这引发了一个关键问题： 高质量奖励信号是否必须依赖外部来源？ 来自南京大学的研究者发现， 一个强大的通用奖励模型并非需要构建，而是可以挖掘出来的 ， 因为它已经潜在地存在于通过标准的下一个 Token 预测训练的任何语言模型中，称之为「内源性奖励（endogenous reward）」。 本文的核心贡献是为这一观点提供严格的理论基础。本文证明了可&hellip;\n">
<style>
    .original-source,
    .original-summary {
        margin-top: 2rem;
        padding: 1rem;
        background-color: #f8f9fa;
        border-left: 4px solid #6c757d;
        border-radius: 4px;
    }
    
    .original-source h4,
    .original-summary h4 {
        margin-top: 0;
        color: #495057;
        font-size: 1.1rem;
    }
    
    .original-source a {
        word-break: break-all;
    }
    
    .original-summary {
        margin-top: 1rem;
    }
</style>


<link rel="stylesheet" href="/css/custom-post.css"> 
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended">
    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#原文摘要">原文摘要</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="">
    <header class="article-header">

    <div class="article-details">
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/%E5%91%A8%E5%BF%97%E5%8D%8E%E5%9B%A2%E9%98%9F%E6%96%B0%E4%BD%9Cllm%E4%B8%AD%E5%AD%98%E5%9C%A8%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%E9%A6%96%E6%AC%A1%E7%90%86%E8%AE%BA%E8%AF%81%E6%98%8Erl%E5%AF%B9llm%E6%9C%89%E6%95%88%E6%80%A7/">周志华团队新作：LLM中存在奖励模型，首次理论证明RL对LLM有效性</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Jul 02, 2025</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    1 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>
    
    <section class="article-content">
    <p>摘要：本文介绍了一种利用大语言模型内部奖励机制解决对齐问题的方法，称为内源性奖励模型。研究表明，这种内源性奖励模型不仅可以超越传统基于人类标注数据的奖励模型，还可以提高模型的性能，改进策略学习过程。</p>
<p>主题标签：人工智能、大数据、机器学习</p>
<h2 id="原文摘要">原文摘要
</h2><p>将大语言模型（LLMs）与复杂的人类价值观对齐，仍然是 AI 面临的一个核心挑战。当前主要的方法是基于人类反馈的强化学习（RLHF）。该流程依赖于一个通过人类偏好训练的奖励模型来对模型输出进行评分，最终对齐后的 LLM 的质量在根本上取决于该奖励模型的质量。
因此，创建一个先进的奖励模型需要建立庞大且高质量的人类偏好数据集，而这一过程通常既缓慢、昂贵，又难以扩展。 
这种对人类标注数据的依赖促使研究者探索其他对齐方法。一个重要的研究方向是基于 AI 反馈的强化学习（RLAIF）。该方法利用强大的专有大语言模型生成奖励信号或偏好标签，从而规避人类标注需求。虽然成本效益显著，但这些方法缺乏严谨的理论基础，且容易继承评判模型本身的风格偏差与固有偏见。这引发了一个关键问题：
高质量奖励信号是否必须依赖外部来源？
来自南京大学的研究者发现，
一个强大的通用奖励模型并非需要构建，而是可以挖掘出来的
， 因为它已经潜在地存在于通过标准的下一个 Token 预测训练的任何语言模型中，称之为「内源性奖励（endogenous reward）」。
本文的核心贡献是为这一观点提供严格的理论基础。本文证明了可&hellip;</p>
</section>
    
    <footer class="article-footer">
    

    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>

    
    
    
    
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



        <time class="article-time--reading">
            
        </time>
    
</article> 

    

    

     
    
        
    <div class="disqus-container">
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "hugo-theme-stack" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>

<script>
    window.addEventListener('onColorSchemeChange', (e) => {
        if (typeof DISQUS == 'object') {
            DISQUS.reset({
                reload: true
            });
        }
    })
</script>

    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2025 Example Person
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.30.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
